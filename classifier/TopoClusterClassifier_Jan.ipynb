{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification of ATLAS Calorimeter Topo-Clusters (Jan)\n",
    "\n",
    "This is a stripped-down version of Max's re-write of his classifier training notebook, so I have removed *some* functionality like ROC curve scans. These could be added back in the future (using the code from his notebooks, maybe put into our library)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If true, force re-training even if a model already exists. Existing model will be lost!\n",
    "overwriteModel = False\n",
    "\n",
    "# If true, continue training and try to train to the last specified epoch.\n",
    "# If EarlyStopping was used, this may result in trying to further train a \"finished\" network.\n",
    "finishTraining = False\n",
    "\n",
    "plotData = False\n",
    "\n",
    "# If no file extension, uses native TensorFlow format (.tf).\n",
    "# If 'h5', uses HDF5. HDF5 does not work for custom layers/classes! (unless you design them a certain way)\n",
    "file_extension = '.h5'\n",
    "if(file_extension != '' and '.' not in file_extension):\n",
    "    file_extension = '.' + file_extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Navigation:\n",
    "- [Simple feed-forward Neural Network](#Simple-feed-forward-Neural-Network)\n",
    "- [Combination Network](#Combination-Network)\n",
    "- [Convolutional Neural Networks](#Convolutional-Neural-Networks)\n",
    "    - [Single-calo-layer CNN](#Single-calo-layer-CNN's)\n",
    "    -[Combination Network (CNN's)](#Combination-Network:-Take-2)\n",
    "    - [Complex CNN's](#Convolutional-Neural-Networks:-More-complicated-architectures)\n",
    "        - [3 EMB layers, separate](#3-EMB-layers,-separate)\n",
    "        - [All layers, separate](#All-layers,-separate)\n",
    "        - [All layers, merged](#All-layers,-merged)\n",
    "        - [EMB merged + TileBar merged](#EMB-merged-+-TileBar-merged)\n",
    "        - [EMB merged + TileBar depth](#EMB-merged-+-TileBar-depth)\n",
    "- [ResNet](#ResNet)\n",
    "- [Combination Network (ResNet + simple NN)](#Combination-Network:-Take-3)\n",
    "\n",
    "- [Summary Plots](#Summary-Plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries and some constants\n",
    "import os, sys, json, pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import ROOT as rt # I will use this for some plotting\n",
    "import uproot as ur\n",
    "\n",
    "path_prefix = os.getcwd() + '/../'\n",
    "plotpath = path_prefix+'classifier/Plots/'\n",
    "modelpath = path_prefix+'classifier/Models/'\n",
    "# %config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "if(path_prefix not in sys.path): sys.path.append(path_prefix)\n",
    "from util import resolution_util as ru\n",
    "from util import plot_util as pu\n",
    "from util import ml_util as mu\n",
    "from util import qol_util as qu\n",
    "\n",
    "# Custom tensorflow.keras callbacks\n",
    "from util.keras.callbacks import GetCallbacks\n",
    "\n",
    "# Classification-specific utils\n",
    "from util.classification import training_util as ctu\n",
    "from util.classification import plot_util as cpu\n",
    "from util.classification import data_util as cdu\n",
    "\n",
    "# metadata\n",
    "layers = [\"EMB1\", \"EMB2\", \"EMB3\", \"TileBar0\", \"TileBar1\", \"TileBar2\"]\n",
    "cell_size_phi = [0.098, 0.0245, 0.0245, 0.1, 0.1, 0.1]\n",
    "cell_size_eta = [0.0031, 0.025, 0.05, 0.1, 0.1, 0.2]\n",
    "len_phi = [4, 16, 16, 4, 4, 4]\n",
    "len_eta = [128, 16, 8, 4, 4, 2]\n",
    "cell_shapes = {layers[i]:(len_eta[i],len_phi[i]) for i in range(len(layers))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fancy display names for each pion type\n",
    "pi_latex = {\n",
    "    'p0': '\\(\\pi^{0}\\)',\n",
    "    'pp': '\\(\\pi^{\\pm}\\)',\n",
    "}\n",
    "pi_text = {\n",
    "    'p0': 'pi0',\n",
    "    'pp': 'pi+/-'\n",
    "}\n",
    "\n",
    "# Plotting settings\n",
    "# xkcd -- turn this on for fun-looking (but marginally less useful) plots\n",
    "use_xkcd = False\n",
    "if(use_xkcd):\n",
    "    mode = 'light'\n",
    "    plt.xkcd(scale=.75,length=100,randomness=1)\n",
    "    \n",
    "# plotting style -- manages our color palette and object colors\n",
    "mode = 'dark' # for publications, use \"light\"\n",
    "plotstyle = qu.PlotStyle(mode)\n",
    "    \n",
    "# some matplotlib-specific stuff\n",
    "params = {'legend.fontsize': 13,\n",
    "          'axes.labelsize': 18}\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will import our data from the `ROOT` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'pion'\n",
    "\n",
    "if(source == 'pion_legacy'):\n",
    "    inputpath = path_prefix+'data/pion_legacy/'\n",
    "    rootfiles = {\n",
    "        'p0':inputpath + 'pi0.root',\n",
    "        'pp':inputpath + 'pi[pm]*.root'\n",
    "    }\n",
    "    branches = [\n",
    "                'clusterE', 'clusterECalib', \n",
    "                'clusterPt', 'clusterEta', 'clusterPhi', \n",
    "                'cluster_nCells', 'cluster_sumCellE', \n",
    "                'cluster_ENG_CALIB_TOT', 'cluster_EM_PROBABILITY'\n",
    "    ]\n",
    "\n",
    "elif(source == 'pion'):\n",
    "    inputpath = path_prefix+'data/pion/'\n",
    "    rootfiles = {\n",
    "        'p0':inputpath + 'user.angerami.mc16_13TeV.900246.PG_singlepi0_logE0p2to2000.e8312_e7400_s3170_r12383.v01-45-gaa27bcb_OutputStream/*.root',\n",
    "        'pp':inputpath + 'user.angerami.mc16_13TeV.900247.PG_singlepion_logE0p2to2000.e8312_e7400_s3170_r12383.v01-45-gaa27bcb_OutputStream/*.root'\n",
    "    }\n",
    "    branches = [\n",
    "        'cluster_E', 'cluster_E_LCCalib', \n",
    "        'cluster_Pt', 'cluster_Eta', 'cluster_Phi', \n",
    "        'cluster_nCells',\n",
    "        'cluster_ENG_CALIB_TOT', 'cluster_EM_PROBABILITY'\n",
    "    ]\n",
    "    \n",
    "else: assert(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "pdata,pcells = mu.setupPionData(\n",
    "    rootfiles, \n",
    "    branches=branches, \n",
    "    layers=layers, \n",
    "    balance_data=True, \n",
    "    n_max = 300000,\n",
    "    verbose=True,\n",
    "    load=True,\n",
    "    save=True,\n",
    "    filename=inputpath + 'tdata',\n",
    "    match_distribution='cluster_ENG_CALIB_TOT',\n",
    "    match_binning = (2000,0.,2000.),\n",
    "    cut_distribution='cluster_ENG_CALIB_TOT',\n",
    "    cut_value = .2,\n",
    "    cut_type='lower'\n",
    ")\n",
    "\n",
    "total = np.sum([len(x) for x in pdata.values()],dtype=int)\n",
    "for key,frame in pdata.items():\n",
    "    n = len(frame)\n",
    "    print(\"Number of {a:<7} events: {b:>10}\\t({c:.1f}%)\".format(a=pi_text[key], b = n, c = 100. * n / total))\n",
    "print(\"Total: {}\".format(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdata_merged, pcells_merged, plabels = cdu.DataPrep(pdata, pcells, layers, trainfrac=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train the network on $\\pi^+$ and $\\pi^0$ events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a few example images.\n",
    "\n",
    "These are the images that we will use to train our network (together with a few other variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(plotData):\n",
    "    cpu.ImagePlot(\n",
    "        pcells,\n",
    "        cluster=100,\n",
    "        layers=layers,\n",
    "        cell_shapes=cell_shapes,\n",
    "        plotpath=plotpath,\n",
    "        filename='calo_images.png',\n",
    "        plotstyle=plotstyle\n",
    "    )\n",
    "    \n",
    "    scaled_shape = (16,16)\n",
    "    cpu.ImagePlot(\n",
    "        pcells,\n",
    "        cluster=100,\n",
    "        scaled_shape=scaled_shape,\n",
    "        layers=layers,\n",
    "        cell_shapes=cell_shapes,\n",
    "        plotpath=plotpath,\n",
    "        filename='calo_images_{}x{}.png'.format(*scaled_shape),\n",
    "        plotstyle=plotstyle\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a few histograms.\n",
    "\n",
    "These are a bit uglier than the `matplotlib` ones Max made, but it's perhaps even easier to see any differences between $\\pi^\\pm$ and $\\pi^0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt.gStyle.SetOptStat(0)\n",
    "plotstyle.SetStyle()\n",
    "\n",
    "if(plotData):\n",
    "    # For storing histograms and legends, to prevent overwriting. (TODO: Probably better ways to do this in PyROOT)\n",
    "    histos = []\n",
    "    legends = []\n",
    "\n",
    "    # Exhaustive list of quantities we want to plot. Note that some will not be available, depending on dataset provenance/age.\n",
    "    n_bins_small = 20\n",
    "    n_bins_med = 100\n",
    "    n_bins_large = int(2e4)\n",
    "    qtys = [\n",
    "        ('cluster_nCells','Cells/Cluster',(n_bins_med,0,1000)),\n",
    "        ('clusterE','cluster Energy [GeV]',(n_bins_large,1.0e-2,2.0e3)),\n",
    "        ('cluster_E','cluster Energy [GeV]',(n_bins_large,1.0e-2,2.0e3)),\n",
    "        ('clusterEta','cluster #eta',(n_bins_small,-0.8,0.8)),\n",
    "        ('cluster_Eta','cluster #eta',(n_bins_small,-0.8,0.8)),\n",
    "        ('clusterPhi','cluster #phi',(n_bins_small,-4.,4.)),\n",
    "        ('cluster_Phi','cluster #phi',(n_bins_small,-4.,4.)),\n",
    "        ('cluster_EM_PROBABILITY','cluster EMProb',(n_bins_small,0.,1.)),\n",
    "        ('cluster_sumCellE','cluster SumCellE',(n_bins_small,0.,2500.)),\n",
    "        ('cluster_ENG_CALIB_TOT','cluster E^{calib}_{tot}',(n_bins_large,1.0e-2,2.0e3))\n",
    "    ]\n",
    "\n",
    "    log_x = ['clusterE', 'cluster_E', 'cluster_ENG_CALIB_TOT']\n",
    "    log_y = ['cluster_nCells','clusterE', 'cluster_E', 'cluster_EM_PROBABILITY', 'cluster_sumCellE', 'cluster_ENG_CALIB_TOT']\n",
    "\n",
    "    qtys = [x for x in qtys if x[0] in branches]\n",
    "\n",
    "    qty_labels = [x[1] for x in qtys]\n",
    "    qty_ranges = [x[2] for x in qtys]\n",
    "    qtys =       [x[0] for x in qtys]\n",
    "\n",
    "    # Set up a canvas.\n",
    "    plot_size = 500\n",
    "    nx = int(np.ceil(len(qtys) / 2))\n",
    "    ny = 2\n",
    "    n_pad = nx * ny\n",
    "    canvas = rt.TCanvas('cluster_hists','c1',plot_size * nx,plot_size * ny)\n",
    "    canvas.Divide(nx,ny)\n",
    "\n",
    "    colors = {'pp':rt.kRed,'p0':rt.kBlue}\n",
    "    styles = {'pp':3440, 'p0':3404}\n",
    "\n",
    "    n_bins=20\n",
    "    for i, (qty, label, rng) in enumerate(zip(qtys, qty_labels, qty_ranges)):\n",
    "        canvas.cd(i+1)\n",
    "        leg = rt.TLegend(0.7,0.8,0.9,0.9)\n",
    "        for ptype, p in pdata.items():\n",
    "            hist = rt.TH1F('h_'+str(ptype)+'_'+str(qty),'',rng[0],rng[1],rng[2])\n",
    "            for entry in p[qty]: hist.Fill(entry)\n",
    "            integral = hist.Integral()\n",
    "            if(integral != 0): hist.Scale(1./hist.Integral())\n",
    "            hist.SetLineColor(colors[ptype])\n",
    "            hist.SetLineWidth(2)\n",
    "\n",
    "            if(qty not in log_x):\n",
    "                hist.SetFillColorAlpha(colors[ptype],0.5)\n",
    "                hist.SetFillStyle(styles[ptype])\n",
    "\n",
    "            hist.Draw('HIST SAME')\n",
    "            hist.GetXaxis().SetTitle(label)\n",
    "            hist.GetYaxis().SetTitle('Normalised events')\n",
    "            hist.SetMaximum(1.5 * hist.GetMaximum())\n",
    "            leg.AddEntry(hist,pi_latex[ptype],'f')\n",
    "            leg.Draw()\n",
    "            histos.append(hist)\n",
    "            legends.append(leg)\n",
    "\n",
    "        if(qty in log_x): rt.gPad.SetLogx()\n",
    "        if(qty in log_y): rt.gPad.SetLogy()\n",
    "    canvas.Draw()\n",
    "\n",
    "    canvas.SaveAs(plotpath+'hist_pi0_plus_minus.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow/Keras prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngpu = 1\n",
    "gpu_list = [\"/gpu:\"+str(i) for i in range(ngpu)]\n",
    "#gpu_list = [\"/gpu:0\"] #[\"/gpu:0\",\"/gpu:1\",\"/gpu:2\",\"/gpu:3\"]\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # disable some of the tensorflow info printouts, only display errors\n",
    "import tensorflow as tf\n",
    "strategy = tf.distribute.MirroredStrategy(devices=gpu_list)\n",
    "# strategy = tf.distribute.MirroredStrategy()\n",
    "ngpu = strategy.num_replicas_in_sync\n",
    "print ('Number of devices: {}'.format(ngpu))\n",
    "from util.classification.models import baseline_nn_model, baseline_cnn_model, emb_cnn_model, all_cnn_model, merged_cnn_model, merged_cnn_2p_model, resnet, simple_combine_model\n",
    "from util.classification.models_exp import exp_cnn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare some callbacks (originally from our regression notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For storing models and some of their metrics (acc, loss)\n",
    "models = {}\n",
    "model_history = {}\n",
    "model_scores = {}\n",
    "model_performance = {}\n",
    "\n",
    "# For storing info on ROC curves\n",
    "roc_fpr = {}\n",
    "roc_tpr = {}\n",
    "roc_thresh = {}\n",
    "roc_auc = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's add some info to our dictionaries that corresponds to the existing `EM LC Prob` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_key = 'LC EMProb'\n",
    "model_scores[model_key] = 1-pdata_merged[\"cluster_EM_PROBABILITY\"]\n",
    "cpu.RocCurves(\n",
    "    model_scores,\n",
    "    data_labels=plabels[:,1],\n",
    "    indices=pdata_merged.test, \n",
    "    roc_fpr=roc_fpr, \n",
    "    roc_tpr=roc_tpr, \n",
    "    roc_thresh=roc_thresh, \n",
    "    roc_auc=roc_auc, \n",
    "    drawPlots=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple feed-forward Neural Network\n",
    "<div style=\"text-align: right\"> <a href=\"#Quick-Navigation:\">Top</a> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we're going to train simple, feed-foward neural networks -- one per calo layer. These will be our \"baseline networks\".\n",
    "\n",
    "Note that while for most of the notebook, we'll train one instance of a network per model, whereas here we will explicitly train multiple instances as we're doing one instance *per calo layer*, each for the same model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-5\n",
    "gamma = 0.1\n",
    "min_delta = 0.0001\n",
    "patience = 3\n",
    "dropout = 0.1 # < 0 -> no dropout\n",
    "normalization = True # normalize calo images to unit integral\n",
    "\n",
    "nepochs = 5\n",
    "batch_size = 200 * ngpu\n",
    "verbose = 1 # 2 for a lot of printouts\n",
    "\n",
    "model_name = 'flat'\n",
    "model_dir = modelpath + model_name # directory for loading/saving flat models\n",
    "\n",
    "for layer in layers:\n",
    "    \n",
    "    model_key = '{}_{}'.format(model_name, layer)\n",
    "    modelfile = '{}/{}{}'.format(model_dir,model_key, file_extension)\n",
    "    npix = cell_shapes[layer][0] * cell_shapes[layer][1]\n",
    "    models[model_key] = baseline_nn_model(strategy, npix, lr=lr, dropout=dropout, normalization=normalization)\n",
    "    \n",
    "    # train the network\n",
    "    models[model_key], model_history[model_key] = ctu.TrainNetwork(\n",
    "        model=models[model_key], \n",
    "        modelfile=modelfile, \n",
    "        x_train = pcells_merged[layer][pdata_merged.train], \n",
    "        y_train = plabels[pdata_merged.train], \n",
    "        x_valid = pcells_merged[layer][pdata_merged.val], \n",
    "        y_valid = plabels[pdata_merged.val], \n",
    "        callbacks = GetCallbacks(modelfile, append=True, use_decay=True, gamma=gamma, min_delta=min_delta, patience=patience), \n",
    "        epochs=nepochs, \n",
    "        batch_size=batch_size, \n",
    "        verbose=verbose, \n",
    "        overwriteModel=overwriteModel,\n",
    "        finishTraining=finishTraining\n",
    "    )\n",
    "        \n",
    "    # get performance metric from test set\n",
    "    model_performance[model_key] = models[model_key].evaluate(\n",
    "        pcells_merged[layer][pdata_merged.test],\n",
    "        plabels[pdata_merged.test],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    print('Finished layer {}.'.format(layer))\n",
    "    \n",
    "    # get/recalculate network scores for the dataset\n",
    "    model_scores[model_key] = models[model_key].predict(pcells_merged[layer])[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at accuracy and loss, as well as ROC curves, for each network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu.MetricPlot(\n",
    "    model_history,\n",
    "    plotpath=plotpath,\n",
    "    plotstyle=plotstyle\n",
    ")\n",
    "\n",
    "cpu.RocCurves(\n",
    "    model_scores,\n",
    "    data_labels=plabels[:,1],\n",
    "    indices=pdata_merged.test, \n",
    "    roc_fpr=roc_fpr, \n",
    "    roc_tpr=roc_tpr, \n",
    "    roc_thresh=roc_thresh, \n",
    "    roc_auc=roc_auc, \n",
    "    plotpath=plotpath,\n",
    "    plotname='ROC_flat',\n",
    "    drawPlots=True,\n",
    "    plotstyle=plotstyle,\n",
    "    figsize=(30,10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combination Network\n",
    "<div style=\"text-align: right\"> <a href=\"#Quick-Navigation:\">Top</a> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we train a simple combination network. Its inputs will be the *outputs* of our simple, feed-forward neural networks from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'simple'\n",
    "model_dir = modelpath + model_name # directory for loading/saving simple combination network\n",
    "model_key = 'simpleCombine'\n",
    "\n",
    "model_scores_stack = np.column_stack( [model_scores['flat_{}'.format(layer)] for layer in layers])\n",
    "n_input = model_scores_stack.shape[1]\n",
    "\n",
    "lr = 1e-3\n",
    "gamma = 0.1\n",
    "min_delta = 0.0005\n",
    "patience = 5\n",
    "\n",
    "nepochs = 100\n",
    "batch_size = 200 * ngpu\n",
    "verbose = 1 # 2 for a lot of printouts\n",
    "    \n",
    "modelfile = '{}/{}{}'.format(model_dir,model_key,file_extension)\n",
    "models[model_key] = simple_combine_model(strategy, lr=lr, n_input=n_input)\n",
    "    \n",
    "# train the network\n",
    "models[model_key], model_history[model_key] = ctu.TrainNetwork(\n",
    "    model=models[model_key], \n",
    "    modelfile=modelfile, \n",
    "    x_train = model_scores_stack[pdata_merged.train],\n",
    "    y_train = plabels[pdata_merged.train], \n",
    "    x_valid = model_scores_stack[pdata_merged.val], \n",
    "    y_valid = plabels[pdata_merged.val], \n",
    "    callbacks = GetCallbacks(modelfile, append=True, use_decay=True, gamma=gamma, min_delta=min_delta, patience=patience), \n",
    "    epochs=nepochs, \n",
    "    batch_size=batch_size, \n",
    "    verbose=verbose, \n",
    "    overwriteModel=overwriteModel,\n",
    "    finishTraining=finishTraining\n",
    ")\n",
    "        \n",
    "# get performance metric from test set\n",
    "model_performance[model_key] = models[model_key].evaluate(\n",
    "    model_scores_stack[pdata_merged.test],\n",
    "    plabels[pdata_merged.test],\n",
    "    verbose=0\n",
    ")\n",
    "    \n",
    "# get/recalculate network scores for the dataset\n",
    "model_scores[model_key] = models[model_key].predict(model_scores_stack)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu.MetricPlot(\n",
    "    model_history,\n",
    "    plotpath=plotpath,\n",
    "    model_keys = ['simpleCombine'],\n",
    "    plotstyle=plotstyle\n",
    ")\n",
    "\n",
    "cpu.RocCurves(\n",
    "    model_scores,\n",
    "    data_labels=plabels[:,1],\n",
    "    indices=pdata_merged.test, \n",
    "    roc_fpr=roc_fpr, \n",
    "    roc_tpr=roc_tpr, \n",
    "    roc_thresh=roc_thresh, \n",
    "    roc_auc=roc_auc, \n",
    "    plotpath=plotpath,\n",
    "    plotname='ROC_simple',\n",
    "    drawPlots=True,\n",
    "    plotstyle=plotstyle,\n",
    "    figsize=(30,10),\n",
    "    model_keys = ['LC EMProb', 'flat_EMB1', 'simpleCombine']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks\n",
    "<div style=\"text-align: right\"> <a href=\"#Quick-Navigation:\">Top</a> </div>\n",
    "Let's try some convolutional neural networks -- afterwards we'll also try an implementation of ResNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcells_merged_unflattened = cdu.ReshapeImages(pcells_merged, cell_shapes, use_layer_names=True)\n",
    "rn_data = cdu.DictionarySplit(\n",
    "    pcells_merged_unflattened, \n",
    "    train_indices=pdata_merged.train,\n",
    "    validation_indices = pdata_merged.val,\n",
    "    test_indices = pdata_merged.test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-calo-layer CNN's\n",
    "<div style=\"text-align: right\"> <a href=\"#Quick-Navigation:\">Top</a> </div>\n",
    "First up, we can try creating a set of CNN's, where each only uses an image from a single calorimeter layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-5\n",
    "gamma = 0.1\n",
    "min_delta = 0.0001\n",
    "patience = 3\n",
    "augmentation = True\n",
    "nepochs = 100\n",
    "batch_size = 200 * ngpu\n",
    "verbose = 1 # 2 for a lot of printouts\n",
    "\n",
    "model_name = 'cnn'\n",
    "model_dir = modelpath + model_name # directory for loading/saving flat models\n",
    "\n",
    "filters = {\n",
    "    'EMB1': (2,4), \n",
    "    'EMB2': (4,4), \n",
    "    'EMB3': (4,2), \n",
    "    'TileBar0': (2,2), \n",
    "    'TileBar1': (2,2), \n",
    "    'TileBar2': (1,1)\n",
    "}\n",
    "pools = {\n",
    "    'EMB1': (1,1), \n",
    "    'EMB2': (2,2), \n",
    "    'EMB3': (1,1), \n",
    "    'TileBar0': (1,1), \n",
    "    'TileBar1': (1,1), \n",
    "    'TileBar2': (1,1)\n",
    "}\n",
    "\n",
    "for layer in layers:\n",
    "    model_key = '{}_{}'.format(model_name, layer)\n",
    "    modelfile = '{}/{}{}'.format(model_dir,model_key, file_extension)    \n",
    "    models[model_key] = baseline_cnn_model(input_shape=cell_shapes[layer], f=filters[layer], pool=pools[layer], lr=lr, augmentation=augmentation)\n",
    "    \n",
    "    # train the network\n",
    "    models[model_key], model_history[model_key] = ctu.TrainNetwork(\n",
    "        model=models[model_key], \n",
    "        modelfile=modelfile, \n",
    "        x_train = rn_data['train'][layer], \n",
    "        y_train = plabels[pdata_merged.train], \n",
    "        x_valid = rn_data['valid'][layer],\n",
    "        y_valid = plabels[pdata_merged.val], \n",
    "        callbacks = GetCallbacks(modelfile, append=True, use_decay=True, gamma=gamma, min_delta=min_delta, patience=patience), \n",
    "        epochs=nepochs, \n",
    "        batch_size=batch_size, \n",
    "        verbose=verbose, \n",
    "        overwriteModel=overwriteModel,\n",
    "        finishTraining=finishTraining\n",
    "    )\n",
    "        \n",
    "    # get performance metric from test set\n",
    "    model_performance[model_key] = models[model_key].evaluate(\n",
    "        rn_data['test'][layer],\n",
    "        plabels[pdata_merged.test],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    print('Finished layer {}.'.format(layer))\n",
    "    \n",
    "    # get/recalculate network scores for the dataset\n",
    "    model_scores[model_key] = models[model_key].predict(pcells_merged_unflattened[layer])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu.MetricPlot(\n",
    "    model_history,\n",
    "    plotpath=plotpath,\n",
    "    plotstyle=plotstyle,\n",
    "    model_keys = ['cnn_{}'.format(layer) for layer in layers]\n",
    ")\n",
    "\n",
    "cpu.RocCurves(\n",
    "    model_scores,\n",
    "    data_labels=plabels[:,1],\n",
    "    indices=pdata_merged.test, \n",
    "    roc_fpr=roc_fpr, \n",
    "    roc_tpr=roc_tpr, \n",
    "    roc_thresh=roc_thresh, \n",
    "    roc_auc=roc_auc, \n",
    "    plotpath=plotpath,\n",
    "    plotname='ROC_cnn',\n",
    "    drawPlots=True,\n",
    "    plotstyle=plotstyle,\n",
    "    figsize=(30,10),\n",
    "    model_keys = ['cnn_{}'.format(layer) for layer in layers]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combination Network: Take 2\n",
    "<div style=\"text-align: right\"> <a href=\"#Quick-Navigation:\">Top</a> </div>\n",
    "Let's try another combination network, using our single layer CNN's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'simple_cnn'\n",
    "model_dir = modelpath + model_name # directory for loading/saving simple combination network\n",
    "model_key = 'simpleCombine_cnn'\n",
    "\n",
    "model_scores_stack = np.column_stack([model_scores['cnn_{}'.format(layer)] for layer in layers])\n",
    "n_input = model_scores_stack.shape[1]\n",
    "\n",
    "lr = 1e-3\n",
    "gamma = 0.1\n",
    "min_delta = 0.0001\n",
    "patience = 3\n",
    "\n",
    "nepochs = 100\n",
    "batch_size = 200 * ngpu\n",
    "verbose = 1 # 2 for a lot of printouts\n",
    "    \n",
    "modelfile = '{}/{}{}'.format(model_dir,model_key,file_extension)\n",
    "models[model_key] = simple_combine_model(strategy, lr=lr, n_input=n_input)\n",
    "    \n",
    "# train the network\n",
    "models[model_key], model_history[model_key] = ctu.TrainNetwork(\n",
    "    model=models[model_key], \n",
    "    modelfile=modelfile, \n",
    "    x_train = model_scores_stack[pdata_merged.train],\n",
    "    y_train = plabels[pdata_merged.train], \n",
    "    x_valid = model_scores_stack[pdata_merged.val], \n",
    "    y_valid = plabels[pdata_merged.val], \n",
    "    callbacks = GetCallbacks(modelfile, append=True, use_decay=True, gamma=gamma, min_delta=min_delta, patience=patience), \n",
    "    epochs=nepochs, \n",
    "    batch_size=batch_size, \n",
    "    verbose=verbose, \n",
    "    overwriteModel=overwriteModel,\n",
    "    finishTraining=finishTraining\n",
    ")\n",
    "        \n",
    "# get performance metric from test set\n",
    "model_performance[model_key] = models[model_key].evaluate(\n",
    "    model_scores_stack[pdata_merged.test],\n",
    "    plabels[pdata_merged.test],\n",
    "    verbose=0\n",
    ")\n",
    "    \n",
    "# get/recalculate network scores for the dataset\n",
    "model_scores[model_key] = models[model_key].predict(model_scores_stack)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu.MetricPlot(\n",
    "    model_history,\n",
    "    plotpath=plotpath,\n",
    "    plotstyle=plotstyle,\n",
    "    model_keys = ['simpleCombine_cnn']\n",
    ")\n",
    "\n",
    "cpu.RocCurves(\n",
    "    model_scores,\n",
    "    data_labels=plabels[:,1],\n",
    "    indices=pdata_merged.test, \n",
    "    roc_fpr=roc_fpr, \n",
    "    roc_tpr=roc_tpr, \n",
    "    roc_thresh=roc_thresh, \n",
    "    roc_auc=roc_auc, \n",
    "    plotpath=plotpath,\n",
    "    plotname='ROC_simple_cnn',\n",
    "    drawPlots=True,\n",
    "    plotstyle=plotstyle,\n",
    "    figsize=(30,10),\n",
    "    model_keys = ['LC EMProb', 'simpleCombine', 'simpleCombine_cnn']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks: More complicated architectures\n",
    "<div style=\"text-align: right\"> <a href=\"#Quick-Navigation:\">Top</a> </div>\n",
    "We can also consider more complex CNN's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 EMB layers, separate\n",
    "<div style=\"text-align: right\"> <a href=\"#Quick-Navigation:\">Top</a> </div>\n",
    "First, we can try one that uses all three EMB layers as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcells_merged_unflattened = cdu.ReshapeImages(pcells_merged, cell_shapes, use_layer_names=True)\n",
    "# keep only the EMB layers\n",
    "pcells_merged_unflattened = {key:pcells_merged_unflattened[key] for key in ['EMB1','EMB2','EMB3']}\n",
    "\n",
    "rn_data = cdu.DictionarySplit(\n",
    "    pcells_merged_unflattened, \n",
    "    train_indices=pdata_merged.train,\n",
    "    validation_indices = pdata_merged.val,\n",
    "    test_indices = pdata_merged.test\n",
    ")\n",
    "\n",
    "rn_data = {key: list(val.values()) for key,val in rn_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'cnn'\n",
    "model_dir = modelpath + model_name # directory for loading/saving ResNet\n",
    "model_key = 'cnn_EMB_all'\n",
    "\n",
    "lr = 1e-4\n",
    "gamma = 0.05\n",
    "min_delta = 0.001\n",
    "patience = 10\n",
    "augmentation=True\n",
    "\n",
    "nepochs = 100\n",
    "batch_size = 200 * ngpu\n",
    "verbose = 1 # 2 for a lot of printouts\n",
    "    \n",
    "modelfile = '{}/{}{}'.format(model_dir,model_key, file_extension)\n",
    "models[model_key] = model = emb_cnn_model(\n",
    "    lr=lr,\n",
    "    augmentation=augmentation\n",
    ")\n",
    "    \n",
    "# train the network\n",
    "models[model_key], model_history[model_key] = ctu.TrainNetwork(\n",
    "    model=models[model_key], \n",
    "    modelfile=modelfile, \n",
    "    x_train = rn_data['train'], \n",
    "    y_train = plabels[pdata_merged.train], \n",
    "    x_valid = rn_data['valid'], \n",
    "    y_valid = plabels[pdata_merged.val], \n",
    "    callbacks = GetCallbacks(modelfile, append=True, use_decay=True, gamma=gamma, min_delta=min_delta, patience=patience), \n",
    "    epochs=nepochs, \n",
    "    batch_size=batch_size, \n",
    "    verbose=verbose, \n",
    "    overwriteModel=overwriteModel,\n",
    "    finishTraining=finishTraining\n",
    ")\n",
    "        \n",
    "# get performance metric from test set\n",
    "model_performance[model_key] = models[model_key].evaluate(\n",
    "    rn_data['test'],\n",
    "    plabels[pdata_merged.test],\n",
    "    verbose=0\n",
    ")\n",
    "    \n",
    "# get/recalculate network scores for the dataset\n",
    "model_scores[model_key] = models[model_key].predict(pcells_merged_unflattened)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu.MetricPlot(\n",
    "    model_history,\n",
    "    plotpath=plotpath,\n",
    "    model_keys=['cnn_EMB_all'],\n",
    "    plotstyle=plotstyle\n",
    ")\n",
    "\n",
    "cpu.RocCurves(\n",
    "    model_scores,\n",
    "    data_labels=plabels[:,1],\n",
    "    indices=pdata_merged.test, \n",
    "    roc_fpr=roc_fpr, \n",
    "    roc_tpr=roc_tpr, \n",
    "    roc_thresh=roc_thresh, \n",
    "    roc_auc=roc_auc, \n",
    "    plotpath=plotpath,\n",
    "    plotname='ROC_cnn_EMB_all',\n",
    "    drawPlots=True,\n",
    "    plotstyle=plotstyle,\n",
    "    figsize=(30,10),\n",
    "    model_keys=['LC EMProb', 'simpleCombine_cnn', 'cnn_EMB_all']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All layers, separate\n",
    "<div style=\"text-align: right\"> <a href=\"#Quick-Navigation:\">Top</a> </div>\n",
    "Similarly, we can try a network using all six calo layers, as separate 1-channel images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcells_merged_unflattened = cdu.ReshapeImages(pcells_merged, cell_shapes, use_layer_names=True)\n",
    "\n",
    "rn_data = cdu.DictionarySplit(\n",
    "    pcells_merged_unflattened, \n",
    "    train_indices=pdata_merged.train,\n",
    "    validation_indices = pdata_merged.val,\n",
    "    test_indices = pdata_merged.test\n",
    ")\n",
    "\n",
    "rn_data = {key: list(val.values()) for key,val in rn_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'cnn'\n",
    "model_dir = modelpath + model_name # directory for loading/saving ResNet\n",
    "model_key = 'cnn_all'\n",
    "\n",
    "lr = 1e-4\n",
    "gamma = 0.05\n",
    "min_delta = 0.001\n",
    "patience = 10\n",
    "augmentation=True\n",
    "\n",
    "nepochs = 100\n",
    "batch_size = 200 * ngpu\n",
    "verbose = 1 # 2 for a lot of printouts\n",
    "    \n",
    "modelfile = '{}/{}{}'.format(model_dir,model_key, file_extension)\n",
    "models[model_key] = model = all_cnn_model(\n",
    "    lr=lr,\n",
    "    augmentation=augmentation\n",
    ")\n",
    "    \n",
    "# train the network\n",
    "models[model_key], model_history[model_key] = ctu.TrainNetwork(\n",
    "    model=models[model_key], \n",
    "    modelfile=modelfile, \n",
    "    x_train = rn_data['train'], \n",
    "    y_train = plabels[pdata_merged.train], \n",
    "    x_valid = rn_data['valid'], \n",
    "    y_valid = plabels[pdata_merged.val], \n",
    "    callbacks = GetCallbacks(modelfile, append=True, use_decay=True, gamma=gamma, min_delta=min_delta, patience=patience), \n",
    "    epochs=nepochs, \n",
    "    batch_size=batch_size, \n",
    "    verbose=verbose, \n",
    "    overwriteModel=overwriteModel,\n",
    "    finishTraining=finishTraining\n",
    ")\n",
    "        \n",
    "# get performance metric from test set\n",
    "model_performance[model_key] = models[model_key].evaluate(\n",
    "    rn_data['test'],\n",
    "    plabels[pdata_merged.test],\n",
    "    verbose=0\n",
    ")\n",
    "    \n",
    "# get/recalculate network scores for the dataset\n",
    "model_scores[model_key] = models[model_key].predict(pcells_merged_unflattened)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu.MetricPlot(\n",
    "    model_history,\n",
    "    plotpath=plotpath,\n",
    "    model_keys=['cnn_all'],\n",
    "    plotstyle=plotstyle\n",
    ")\n",
    "\n",
    "cpu.RocCurves(\n",
    "    model_scores,\n",
    "    data_labels=plabels[:,1],\n",
    "    indices=pdata_merged.test, \n",
    "    roc_fpr=roc_fpr, \n",
    "    roc_tpr=roc_tpr, \n",
    "    roc_thresh=roc_thresh, \n",
    "    roc_auc=roc_auc, \n",
    "    plotpath=plotpath,\n",
    "    plotname='ROC_cnn_all',\n",
    "    drawPlots=True,\n",
    "    plotstyle=plotstyle,\n",
    "    figsize=(30,10),\n",
    "    model_keys=['LC EMProb', 'simpleCombine_cnn', 'cnn_EMB_all', 'cnn_all']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All layers, merged\n",
    "<div style=\"text-align: right\"> <a href=\"#Quick-Navigation:\">Top</a> </div>\n",
    "Of course, we can also treat the 6 calo layers as 6 channels of the *same* image. Note that this will require some rescaling of the images so that their dimensions match -- we'll also be careful to preserve their integrals. This last point is probably even more relevant for regression tasks (i.e. predicting energy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcells_merged_unflattened = cdu.ReshapeImages(pcells_merged, cell_shapes, use_layer_names=True)\n",
    "rn_data = cdu.DictionarySplit(\n",
    "    pcells_merged_unflattened, \n",
    "    train_indices=pdata_merged.train,\n",
    "    validation_indices = pdata_merged.val,\n",
    "    test_indices = pdata_merged.test\n",
    ")\n",
    "rn_data = {key: list(val.values()) for key,val in rn_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'cnn'\n",
    "model_dir = modelpath + model_name # directory for loading/saving ResNet\n",
    "model_key = 'cnn_merged'\n",
    "\n",
    "input_shape = (16,16)\n",
    "dropout = 0.2\n",
    "\n",
    "lr = 1e-4\n",
    "gamma = 0.05\n",
    "min_delta = 0.001\n",
    "patience = 10\n",
    "augmentation=True\n",
    "\n",
    "nepochs = 100\n",
    "batch_size = 200 * ngpu\n",
    "verbose = 1 # 2 for a lot of printouts\n",
    "    \n",
    "modelfile = '{}/{}{}'.format(model_dir,model_key, file_extension)\n",
    "models[model_key] = model = merged_cnn_model(\n",
    "    lr=lr,\n",
    "    input_shape=input_shape,\n",
    "    dropout=dropout,\n",
    "    augmentation=augmentation\n",
    ")\n",
    "    \n",
    "# train the network\n",
    "models[model_key], model_history[model_key] = ctu.TrainNetwork(\n",
    "    model=models[model_key], \n",
    "    modelfile=modelfile, \n",
    "    x_train = rn_data['train'], \n",
    "    y_train = plabels[pdata_merged.train], \n",
    "    x_valid = rn_data['valid'], \n",
    "    y_valid = plabels[pdata_merged.val], \n",
    "    callbacks = GetCallbacks(modelfile, append=True, use_decay=True, gamma=gamma, min_delta=min_delta, patience=patience), \n",
    "    epochs=nepochs, \n",
    "    batch_size=batch_size, \n",
    "    verbose=verbose, \n",
    "    overwriteModel=overwriteModel,\n",
    "    finishTraining=finishTraining\n",
    ")\n",
    "        \n",
    "# get performance metric from test set\n",
    "model_performance[model_key] = models[model_key].evaluate(\n",
    "    rn_data['test'],\n",
    "    plabels[pdata_merged.test],\n",
    "    verbose=0\n",
    ")\n",
    "    \n",
    "# get/recalculate network scores for the dataset\n",
    "model_scores[model_key] = models[model_key].predict(pcells_merged_unflattened)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu.MetricPlot(\n",
    "    model_history,\n",
    "    plotpath=plotpath,\n",
    "    model_keys=['cnn_merged'],\n",
    "    plotstyle=plotstyle\n",
    ")\n",
    "\n",
    "cpu.RocCurves(\n",
    "    model_scores,\n",
    "    data_labels=plabels[:,1],\n",
    "    indices=pdata_merged.test, \n",
    "    roc_fpr=roc_fpr, \n",
    "    roc_tpr=roc_tpr, \n",
    "    roc_thresh=roc_thresh, \n",
    "    roc_auc=roc_auc, \n",
    "    plotpath=plotpath,\n",
    "    plotname='ROC_cnn_merged',\n",
    "    drawPlots=True,\n",
    "    plotstyle=plotstyle,\n",
    "    figsize=(30,10),\n",
    "    model_keys=['LC EMProb', 'simpleCombine_cnn', 'cnn_EMB_all', 'cnn_all', 'cnn_merged']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EMB merged + TileBar merged\n",
    "<div style=\"text-align: right\"> <a href=\"#Quick-Navigation:\">Top</a> </div>\n",
    "As another variation on this theme, we can try a \"merged\" CNN where we separately treat the 3 EMB layers and the 3 TileBar layers. This will allow us to perform separate rescalings for each -- and we may thus be able to have fewer parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcells_merged_unflattened = cdu.ReshapeImages(pcells_merged, cell_shapes, use_layer_names=True)\n",
    "rn_data = cdu.DictionarySplit(\n",
    "    pcells_merged_unflattened, \n",
    "    train_indices=pdata_merged.train,\n",
    "    validation_indices = pdata_merged.val,\n",
    "    test_indices = pdata_merged.test\n",
    ")\n",
    "rn_data = {key: list(val.values()) for key,val in rn_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'cnn'\n",
    "model_dir = modelpath + model_name # directory for loading/saving ResNet\n",
    "model_key = 'cnn_merged_2p'\n",
    "\n",
    "input_shape1 = (16,16)\n",
    "input_shape2 = (4,4)\n",
    "dropout = 0.2\n",
    "\n",
    "lr = 1e-4\n",
    "gamma = 0.05\n",
    "min_delta = 0.001\n",
    "patience = 10\n",
    "augmentation=True\n",
    "\n",
    "nepochs = 100\n",
    "batch_size = 200 * ngpu\n",
    "verbose = 1 # 2 for a lot of printouts\n",
    "    \n",
    "modelfile = '{}/{}{}'.format(model_dir,model_key, file_extension)\n",
    "models[model_key] = model = merged_cnn_2p_model(\n",
    "    lr=lr,\n",
    "    input_shape1=input_shape1,\n",
    "    input_shape2=input_shape2,\n",
    "    dropout=dropout,\n",
    "    augmentation=augmentation\n",
    ")\n",
    "    \n",
    "# train the network\n",
    "models[model_key], model_history[model_key] = ctu.TrainNetwork(\n",
    "    model=models[model_key], \n",
    "    modelfile=modelfile, \n",
    "    x_train = rn_data['train'], \n",
    "    y_train = plabels[pdata_merged.train], \n",
    "    x_valid = rn_data['valid'], \n",
    "    y_valid = plabels[pdata_merged.val], \n",
    "    callbacks = GetCallbacks(modelfile, append=True, use_decay=True, gamma=gamma, min_delta=min_delta, patience=patience), \n",
    "    epochs=nepochs, \n",
    "    batch_size=batch_size, \n",
    "    verbose=verbose, \n",
    "    overwriteModel=overwriteModel,\n",
    "    finishTraining=finishTraining\n",
    ")\n",
    "        \n",
    "# get performance metric from test set\n",
    "model_performance[model_key] = models[model_key].evaluate(\n",
    "    rn_data['test'],\n",
    "    plabels[pdata_merged.test],\n",
    "    verbose=0\n",
    ")\n",
    "    \n",
    "# get/recalculate network scores for the dataset\n",
    "model_scores[model_key] = models[model_key].predict(pcells_merged_unflattened)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu.MetricPlot(\n",
    "    model_history,\n",
    "    plotpath=plotpath,\n",
    "    model_keys=['cnn_merged_2p'],\n",
    "    plotstyle=plotstyle\n",
    ")\n",
    "\n",
    "cpu.RocCurves(\n",
    "    model_scores,\n",
    "    data_labels=plabels[:,1],\n",
    "    indices=pdata_merged.test, \n",
    "    roc_fpr=roc_fpr, \n",
    "    roc_tpr=roc_tpr, \n",
    "    roc_thresh=roc_thresh, \n",
    "    roc_auc=roc_auc, \n",
    "    plotpath=plotpath,\n",
    "    plotname='ROC_cnn_merged_2p',\n",
    "    drawPlots=True,\n",
    "    plotstyle=plotstyle,\n",
    "    figsize=(30,10),\n",
    "    model_keys=['LC EMProb', 'cnn_merged', 'cnn_merged_2p']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EMB merged + TileBar depth\n",
    "<div style=\"text-align: right\"> <a href=\"#Quick-Navigation:\">Top</a> </div>\n",
    "As an experiment, we can try a network where we use a CNN approach for the EMB layers, whereas for TileBar we just use the integrals of the 3 Tilebar layers. In other words, we don't fully use TileBar images, but just get some depth info from how populated they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcells_merged_unflattened = cdu.ReshapeImages(pcells_merged, cell_shapes, use_layer_names=True)\n",
    "rn_data = cdu.DictionarySplit(\n",
    "    pcells_merged_unflattened, \n",
    "    train_indices=pdata_merged.train,\n",
    "    validation_indices = pdata_merged.val,\n",
    "    test_indices = pdata_merged.test\n",
    ")\n",
    "rn_data = {key: list(val.values()) for key,val in rn_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'cnn'\n",
    "model_dir = modelpath + model_name # directory for loading/saving ResNet\n",
    "model_key = 'cnn_exp'\n",
    "\n",
    "input_shape = (16,16)\n",
    "dropout = 0.2\n",
    "\n",
    "lr = 1e-4\n",
    "gamma = 0.05\n",
    "min_delta = 0.001\n",
    "patience = 10\n",
    "augmentation=True\n",
    "\n",
    "nepochs = 100\n",
    "batch_size = 200 * ngpu\n",
    "verbose = 1 # 2 for a lot of printouts\n",
    "    \n",
    "modelfile = '{}/{}{}'.format(model_dir,model_key, file_extension)\n",
    "models[model_key] = model = exp_cnn_model(\n",
    "    lr=lr,\n",
    "    input_shape=input_shape,\n",
    "    dropout=dropout,\n",
    "    augmentation=augmentation\n",
    ")\n",
    "    \n",
    "# train the network\n",
    "models[model_key], model_history[model_key] = ctu.TrainNetwork(\n",
    "    model=models[model_key], \n",
    "    modelfile=modelfile, \n",
    "    x_train = rn_data['train'], \n",
    "    y_train = plabels[pdata_merged.train], \n",
    "    x_valid = rn_data['valid'], \n",
    "    y_valid = plabels[pdata_merged.val], \n",
    "    callbacks = GetCallbacks(modelfile, append=True, use_decay=True, gamma=gamma, min_delta=min_delta, patience=patience), \n",
    "    epochs=nepochs, \n",
    "    batch_size=batch_size, \n",
    "    verbose=verbose, \n",
    "    overwriteModel=overwriteModel,\n",
    "    finishTraining=finishTraining\n",
    ")\n",
    "        \n",
    "# get performance metric from test set\n",
    "model_performance[model_key] = models[model_key].evaluate(\n",
    "    rn_data['test'],\n",
    "    plabels[pdata_merged.test],\n",
    "    verbose=0\n",
    ")\n",
    "    \n",
    "# get/recalculate network scores for the dataset\n",
    "model_scores[model_key] = models[model_key].predict(pcells_merged_unflattened)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu.MetricPlot(\n",
    "    model_history,\n",
    "    plotpath=plotpath,\n",
    "    model_keys=['cnn_exp'],\n",
    "    plotstyle=plotstyle\n",
    ")\n",
    "\n",
    "cpu.RocCurves(\n",
    "    model_scores,\n",
    "    data_labels=plabels[:,1],\n",
    "    indices=pdata_merged.test, \n",
    "    roc_fpr=roc_fpr, \n",
    "    roc_tpr=roc_tpr, \n",
    "    roc_thresh=roc_thresh, \n",
    "    roc_auc=roc_auc, \n",
    "    plotpath=plotpath,\n",
    "    plotname='ROC_cnn_exp',\n",
    "    drawPlots=True,\n",
    "    plotstyle=plotstyle,\n",
    "    figsize=(30,10),\n",
    "    model_keys=['LC EMProb', 'cnn_merged', 'cnn_merged_2p', 'cnn_exp']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet\n",
    "<div style=\"text-align: right\"> <a href=\"#Quick-Navigation:\">Top</a> </div>\n",
    "We can also train an instance of ResNet. As with our multi-layer CNN's, we'll need to perform some up/downscaling of images, so that they all have the same dimensions. As we did for some of our CNN's, we will use the maximum possible granularity, `(128,16)` (corresponding with maximum eta and phi dimensions among all calo layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcells_merged_unflattened = cdu.ReshapeImages(pcells_merged, cell_shapes, use_layer_names=False)\n",
    "rn_data = cdu.DictionarySplit(\n",
    "    pcells_merged_unflattened, \n",
    "    train_indices=pdata_merged.train,\n",
    "    validation_indices = pdata_merged.val,\n",
    "    test_indices = pdata_merged.test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run a quick test to make sure that our image scaling is working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_test=False\n",
    "\n",
    "from util.keras.layers import ImageScaleBlock\n",
    "def TestImage(idxs):\n",
    "    images = [np.stack([rn_train['input{}'.format(x)][idx,:,:] for idx in idxs],axis=0) for x in range(6)]    \n",
    "    # need to add the last dimension, corresponding with channel (will be of size 1)\n",
    "    images = [np.expand_dims(im, axis=-1) for im in images]\n",
    "    return images\n",
    "\n",
    "if(image_test):\n",
    "    test_idx = np.arange(5)\n",
    "    image = TestImage(test_idx)\n",
    "    scaled_image = ImageScaleBlock((128,16),normalization=True)(image).numpy()\n",
    "    for i,im in enumerate(image):\n",
    "        integrals_old = np.sum(im,axis=(1,2)).flatten()\n",
    "        integrals_new = np.sum(scaled_image[:,:,:,i], axis=(1,2)).flatten()\n",
    "\n",
    "        ratio = integrals_old.copy()\n",
    "        ratio[ratio==0.] = 1.\n",
    "        ratio = integrals_new/ratio\n",
    "\n",
    "        integrals_old = '\\t\\t'.join(['{:.1e}'.format(x) for x in integrals_old])\n",
    "        integrals_new = '\\t\\t'.join(['{:.1e}'.format(x) for x in integrals_new])\n",
    "        ratio         = '\\t\\t'.join(['{:.1e}'.format(x) for x in ratio        ])\n",
    "\n",
    "        print('Integral {} = {}'.format(i,integrals_old))\n",
    "        print('           = {}'.format(integrals_new))\n",
    "        print('Ratios     = {}'.format(ratio))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.backend.set_image_data_format('channels_last')\n",
    "model_name = 'resnet'\n",
    "model_dir = modelpath + model_name # directory for loading/saving ResNet\n",
    "model_key = 'resnet'\n",
    "\n",
    "lr = 1e-4\n",
    "gamma = 0.05\n",
    "min_delta = 0.001\n",
    "patience = 20 # large patience since loss sometimes fluctuates upwards for a bit?\n",
    "input_shape = (128,16)\n",
    "channels = 6\n",
    "augmentation=True\n",
    "normalization=True\n",
    "filter_sets = [\n",
    "    [64,64,256],\n",
    "    [128,128,512]\n",
    "    #[256,256,1024],\n",
    "    #[512,512,2048]\n",
    "]         \n",
    "f_vals = [3,3] # [3,3,3,3] sizes of filters in middle of conv/identity blocks\n",
    "s_vals = [1,2] # [1,2,2,2] strides for each convolutional block\n",
    "i_vals = [2,3] # [2,3,5,2] number of identity blocks per stage\n",
    "\n",
    "nepochs = 200\n",
    "batch_size = 50 * ngpu\n",
    "verbose = 1 # 2 for a lot of printouts\n",
    "    \n",
    "modelfile = '{}/{}{}'.format(model_dir,model_key, file_extension)\n",
    "models[model_key] = model = resnet(\n",
    "    filter_sets=filter_sets, \n",
    "    lr=lr, \n",
    "    channels=channels, \n",
    "    f_vals=f_vals, \n",
    "    s_vals=s_vals, \n",
    "    i_vals=i_vals, \n",
    "    input_shape=input_shape, \n",
    "    augmentation=augmentation,\n",
    "    normalization=normalization\n",
    ")\n",
    "    \n",
    "# train the network\n",
    "models[model_key], model_history[model_key] = ctu.TrainNetwork(\n",
    "    model=models[model_key], \n",
    "    modelfile=modelfile, \n",
    "    x_train = rn_data['train'], \n",
    "    y_train = plabels[pdata_merged.train], \n",
    "    x_valid = rn_data['valid'], \n",
    "    y_valid = plabels[pdata_merged.val], \n",
    "    callbacks = GetCallbacks(modelfile, append=True, use_decay=True, gamma=gamma, min_delta=min_delta, patience=patience), \n",
    "    epochs=nepochs, \n",
    "    batch_size=batch_size, \n",
    "    verbose=verbose, \n",
    "    overwriteModel=overwriteModel,\n",
    "    finishTraining=finishTraining\n",
    ")\n",
    "        \n",
    "# get performance metric from test set\n",
    "model_performance[model_key] = models[model_key].evaluate(\n",
    "    rn_data['test'],\n",
    "    plabels[pdata_merged.test],\n",
    "    verbose=0\n",
    ")\n",
    "    \n",
    "# get/recalculate network scores for the dataset\n",
    "model_scores[model_key] = models[model_key].predict(pcells_merged_unflattened)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu.MetricPlot(\n",
    "    model_history,\n",
    "    plotpath=plotpath,\n",
    "    model_keys=['resnet'],\n",
    "    plotstyle=plotstyle\n",
    ")\n",
    "\n",
    "cpu.RocCurves(\n",
    "    model_scores,\n",
    "    data_labels=plabels[:,1],\n",
    "    indices=pdata_merged.test, \n",
    "    roc_fpr=roc_fpr, \n",
    "    roc_tpr=roc_tpr, \n",
    "    roc_thresh=roc_thresh, \n",
    "    roc_auc=roc_auc, \n",
    "    plotpath=plotpath,\n",
    "    plotname='ROC_resnet',\n",
    "    drawPlots=True,\n",
    "    plotstyle=plotstyle,\n",
    "    figsize=(30,10),\n",
    "    model_keys=['LC EMProb', 'flat_EMB1', 'resnet']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combination Network: Take 3\n",
    "<div style=\"text-align: right\"> <a href=\"#Quick-Navigation:\">Top</a> </div>\n",
    "For fun, we can also train a combination network where we use our flat DNN's, and ResNet as an additional input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores_stack = np.column_stack( [model_scores['flat_{}'.format(layer)] for layer in layers])\n",
    "model_scores_stack = np.column_stack((model_scores_stack, model_scores['resnet']))\n",
    "n_input = model_scores_stack.shape[1]\n",
    "\n",
    "nepochs = 50\n",
    "batch_size = 200*ngpu\n",
    "verbose = 2\n",
    "\n",
    "\n",
    "model_name = 'simple_resnet'\n",
    "model_dir = modelpath + model_name # directory for loading/saving ResNet\n",
    "model_key = 'simpleCombine_resnet'\n",
    "\n",
    "lr = 1e-3\n",
    "gamma = 0.1\n",
    "min_delta = 0.001\n",
    "patience = 5\n",
    "\n",
    "nepochs = 100\n",
    "batch_size = 200 * ngpu\n",
    "verbose = 1 # 2 for a lot of printouts\n",
    "    \n",
    "modelfile = '{}/{}{}'.format(model_dir,model_key, file_extension)\n",
    "models[model_key] = simple_combine_model(strategy, lr=lr, n_input=n_input)\n",
    "    \n",
    "# train the network\n",
    "models[model_key], model_history[model_key] = ctu.TrainNetwork(\n",
    "    model=models[model_key], \n",
    "    modelfile=modelfile, \n",
    "    x_train = model_scores_stack[pdata_merged.train],\n",
    "    y_train = plabels[pdata_merged.train], \n",
    "    x_valid = model_scores_stack[pdata_merged.val], \n",
    "    y_valid = plabels[pdata_merged.val], \n",
    "    callbacks = GetCallbacks(modelfile, append=True, use_decay=True, gamma=gamma, min_delta=min_delta, patience=patience), \n",
    "    epochs=nepochs, \n",
    "    batch_size=batch_size, \n",
    "    verbose=verbose, \n",
    "    overwriteModel=overwriteModel,\n",
    "    finishTraining=finishTraining\n",
    ")\n",
    "        \n",
    "# get performance metric from test set\n",
    "model_performance[model_key] = models[model_key].evaluate(\n",
    "    model_scores_stack[pdata_merged.test],\n",
    "    plabels[pdata_merged.test],\n",
    "    verbose=0\n",
    ")\n",
    "    \n",
    "# get/recalculate network scores for the dataset\n",
    "model_scores[model_key] = models[model_key].predict(model_scores_stack)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu.MetricPlot(\n",
    "    model_history,\n",
    "    plotpath=plotpath,\n",
    "    model_keys = ['simpleCombine_resnet'],\n",
    "    plotstyle=plotstyle\n",
    ")\n",
    "\n",
    "cpu.RocCurves(\n",
    "    model_scores,\n",
    "    data_labels=plabels[:,1],\n",
    "    indices=pdata_merged.test, \n",
    "    roc_fpr=roc_fpr, \n",
    "    roc_tpr=roc_tpr, \n",
    "    roc_thresh=roc_thresh, \n",
    "    roc_auc=roc_auc, \n",
    "    plotpath=plotpath,\n",
    "    plotname='ROC_simple_resnet',\n",
    "    drawPlots=True,\n",
    "    plotstyle=plotstyle,\n",
    "    figsize=(30,10),\n",
    "    model_keys = ['LC EMProb', 'simpleCombine', 'resnet', 'simpleCombine_cnn', 'simpleCombine_resnet']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Plots\n",
    "<div style=\"text-align: right\"> <a href=\"#Quick-Navigation:\">Top</a> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've trained networks to our hearts' content, we can make a set of ROC curves to summarize our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu.RocCurves(\n",
    "    model_scores,\n",
    "    data_labels=plabels[:,1],\n",
    "    indices=pdata_merged.test, \n",
    "    roc_fpr=roc_fpr, \n",
    "    roc_tpr=roc_tpr, \n",
    "    roc_thresh=roc_thresh, \n",
    "    roc_auc=roc_auc, \n",
    "    plotpath=plotpath,\n",
    "    plotname='ROC_summary',\n",
    "    drawPlots=True,\n",
    "    plotstyle=plotstyle,\n",
    "    figsize=(30,10),\n",
    "    model_keys = ['LC EMProb', 'flat_EMB1', 'flat_EMB2', 'flat_EMB3', 'cnn_EMB1', 'cnn_EMB2', 'cnn_EMB3', 'simpleCombine', 'simpleCombine_cnn', 'cnn_EMB_all', 'cnn_all', 'cnn_merged', 'resnet', 'simpleCombine_resnet']\n",
    ")\n",
    "\n",
    "cpu.RocCurves(\n",
    "    model_scores,\n",
    "    data_labels=plabels[:,1],\n",
    "    indices=pdata_merged.test, \n",
    "    roc_fpr=roc_fpr, \n",
    "    roc_tpr=roc_tpr, \n",
    "    roc_thresh=roc_thresh, \n",
    "    roc_auc=roc_auc, \n",
    "    plotpath=plotpath,\n",
    "    plotname='ROC_best',\n",
    "    drawPlots=True,\n",
    "    plotstyle=plotstyle,\n",
    "    figsize=(30,10),\n",
    "    model_keys = ['LC EMProb', 'flat_EMB1', 'simpleCombine_cnn', 'simpleCombine', 'cnn_EMB_all', 'cnn_all', 'cnn_merged', 'resnet', 'simpleCombine_resnet']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also make a scatterplot of number of learnable parameters versus accuracy. The number of parameters can be thought of as a stand-in for complexity (though I assume that computational complexity/cost will also depend on the exact operations being performed).\n",
    "\n",
    "Note that there is some inherent hard-coding being done here, since some of our models (e.g. `simpleCombine`) use other models as input -- and for such models, we want to count their inputs' weights plus their own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "complexity_scatter = {}\n",
    "\n",
    "for key, model in models.items():\n",
    "    trainable_count = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
    "    accuracy        = model_performance[key][1]\n",
    "    auc             = roc_auc[key]\n",
    "    complexity_scatter[key] = [trainable_count, accuracy, auc]\n",
    "    \n",
    "# manually adjust complexity_scatter for our combination networks\n",
    "\n",
    "flat_weights = np.sum([np.sum([K.count_params(w) for w in models[key].trainable_weights]) for key in ['flat_{}'.format(layer) for layer in layers]])\n",
    "cnn_weights = np.sum([np.sum([K.count_params(w) for w in models[key].trainable_weights]) for key in ['cnn_{}'.format(layer) for layer in layers]])\n",
    "\n",
    "complexity_scatter['simpleCombine'][0] += flat_weights\n",
    "complexity_scatter['simpleCombine_cnn'][0] += cnn_weights\n",
    "complexity_scatter['simpleCombine_resnet'][0] += flat_weights + complexity_scatter['resnet'][0]\n",
    "    \n",
    "fig, ax = plt.subplots(1,2, figsize=(25,10))\n",
    "for axis in ax: plotstyle.SetStylePlt(axis)\n",
    "\n",
    "# Make a scatter -- there will be a lot of points, so consider not drawing them for each network.\n",
    "draw_keys = ['flat_EMB1', 'simpleCombine_cnn', 'simpleCombine', 'cnn_EMB_all', 'cnn_all', 'cnn_merged', 'resnet', 'simpleCombine_resnet']\n",
    "draw_cols = [plotstyle.colors[(i+1) % (len(plotstyle.colors)-1)] for i in range(len(draw_keys))]\n",
    "\n",
    "for i,key in enumerate(draw_keys):\n",
    "    if(key not in complexity_scatter.keys()): continue\n",
    "    x,y1,y2 = complexity_scatter[key]\n",
    "    ax[0].scatter(x=x,y=y1, label=key, color=draw_cols[i], s=64)\n",
    "    ax[1].scatter(x=x,y=y2, label=key, color=draw_cols[i], s=64)\n",
    "    \n",
    "    ax[0].set_xlabel('# Learnable Parameters')\n",
    "    ax[1].set_xlabel('# Learnable Parameters')\n",
    "    \n",
    "    ax[0].set_ylabel('Accuracy')\n",
    "    ax[1].set_ylabel('Area under ROC curve')\n",
    "    \n",
    "    ax[0].set_xlim([0.,2.5e6])\n",
    "    ax[1].set_xlim([0.,2.5e6])\n",
    "\n",
    "legs = []\n",
    "for i,axis in enumerate(ax):\n",
    "    legs.append(axis.legend(facecolor=plotstyle.canv_plt))\n",
    "    for leg_text in legs[-1].get_texts(): leg_text.set_color(plotstyle.text_plt)\n",
    "        \n",
    "qu.SaveSubplots(fig, ax, ['acc_params', 'auc_params'], savedir=plotpath, ps=plotstyle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these plots, `cnn_all` looks like a relatively appealing option -- it's one of the highest-performing networks, and it actually has the fewest learnable parameters. `resnet` performs slightly better, but at the cost of many more learnable weights. This added complexity may translate to slower deployment time, and cause issues if we try to use this network in a triggering context (i.e. on an FPGA), where we will have to deal with significant hardware and timing constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be nice to also make a table (or plot) depicting the false positive rate at different true positive rate (i.e. pick out a few reference points from the ROC curves)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "fpr_data = {}\n",
    "tpr_vals = np.array([0.6,0.8,0.9,0.95])\n",
    "\n",
    "for tpr_val in tpr_vals: \n",
    "    fpr_data[tpr_val] = {}\n",
    "    for i, draw_key in enumerate(draw_keys):\n",
    "        \n",
    "        # Want to find the fpr at the given tpr\n",
    "        fpr = roc_fpr[draw_key]\n",
    "        tpr = roc_tpr[draw_key]\n",
    "        f = interp1d(tpr, fpr)        \n",
    "        fpr_data[tpr_val][draw_key] = f(tpr_val)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "plotstyle.SetStylePlt(ax)\n",
    "\n",
    "# multiple bars -- handle visual offsets\n",
    "bar_width = 0.2\n",
    "n = len(tpr_vals)\n",
    "offsets = np.linspace(0.,(n-1) * bar_width,n)\n",
    "offsets = offsets - np.mean(offsets)\n",
    "\n",
    "# hatches for distinguishing between different tpr values\n",
    "hatches = ['/','\\\\','//','\\\\\\\\']\n",
    "\n",
    "x = np.arange(len(draw_keys))\n",
    "rects = []\n",
    "for i,tpr_val in enumerate(fpr_data.keys()):\n",
    "    bar = ax.bar(\n",
    "        x + offsets[i], \n",
    "        fpr_data[tpr_val].values(),\n",
    "        bar_width,\n",
    "        label='@ True positive rate = {:.2f}'.format(tpr_val), \n",
    "        color=draw_cols,\n",
    "        hatch = hatches[i],\n",
    "        edgecolor=plotstyle.text_plt\n",
    "    )\n",
    "    \n",
    "    for rect in bar:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{:.2f}'.format(height),\n",
    "            xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "            xytext=(0, 3),  # 3 points vertical offset\n",
    "            textcoords=\"offset points\",\n",
    "            color=plotstyle.text_plt,\n",
    "            ha='center', va='bottom'\n",
    "                   )\n",
    "    \n",
    "    rects.append(bar)\n",
    "\n",
    "result = ax.set_xticks(x)\n",
    "result = ax.set_xticklabels(draw_keys)\n",
    "\n",
    "ax.set_xlabel('Network')\n",
    "ax.set_ylabel('False positive rate')\n",
    "\n",
    "leg = ax.legend(facecolor=plotstyle.canv_plt)\n",
    "for leg_text in leg.get_texts(): leg_text.set_color(plotstyle.text_plt)\n",
    "    \n",
    "qu.SaveSubplots(fig, np.array([ax]), ['fpr'], savedir=plotpath, ps=plotstyle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
