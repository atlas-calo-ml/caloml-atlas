{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7275ef37-bf27-47c0-8874-27a01a548ca5",
   "metadata": {},
   "source": [
    "## Cluster Studies (Jan)\n",
    "\n",
    "Here is an updated version of the ClusterStudies.ipynb notebook, using uproot4 and newer versions of our plotting utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bd3246-1e22-469c-9e14-dc2cd257e21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries and some constants\n",
    "import os, sys, pathlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import ROOT as rt # I will use this for some plotting\n",
    "import uproot as ur\n",
    "from numba import njit\n",
    "\n",
    "path_prefix = os.getcwd() + '/../'\n",
    "modelpath = path_prefix+'clusters/Models/'\n",
    "\n",
    "if(path_prefix not in sys.path): sys.path.append(path_prefix)\n",
    "from util import resolution_util as ru\n",
    "from util import plot_util as pu\n",
    "from util import ml_util as mu\n",
    "from util import qol_util as qu\n",
    "\n",
    "# metadata\n",
    "layers = [\"EMB1\", \"EMB2\", \"EMB3\", \"TileBar0\", \"TileBar1\", \"TileBar2\"]\n",
    "cell_size_phi = [0.098, 0.0245, 0.0245, 0.1, 0.1, 0.1]\n",
    "cell_size_eta = [0.0031, 0.025, 0.05, 0.1, 0.1, 0.2]\n",
    "len_phi = [4, 16, 16, 4, 4, 4]\n",
    "len_eta = [128, 16, 8, 4, 4, 2]\n",
    "cell_shapes = {layers[i]:(len_eta[i],len_phi[i]) for i in range(len(layers))}\n",
    "cell_widths = {layers[i]:(cell_size_eta[i],cell_size_phi[i]) for i in range(len(layers))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695ab5e9-7527-493c-8107-29f6076957a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fancy display names for each pion type\n",
    "pi_latex = {\n",
    "    'p0': '#pi^{0}',\n",
    "    'pp': '#pi^{#pm}',\n",
    "}\n",
    "\n",
    "pi_latex_plt = {\n",
    "    'p0': '$\\pi^{0}$',\n",
    "    'pp': '$\\pi^{\\pm}$'\n",
    "}\n",
    "\n",
    "# Plotting settings\n",
    "# xkcd -- turn this on for fun-looking (but marginally less useful) plots\n",
    "use_xkcd = False\n",
    "if(use_xkcd):\n",
    "    mode = 'light'\n",
    "    plt.xkcd(scale=.75,length=100,randomness=1)\n",
    "    \n",
    "# plotting style -- manages our color palette and object colors\n",
    "mode = 'dark' # for publications, use \"light\"\n",
    "plotstyle = qu.PlotStyle(mode)\n",
    "plotstyle.SetStyle()\n",
    "    \n",
    "# some matplotlib-specific stuff\n",
    "params = {'legend.fontsize': 13,\n",
    "          'axes.labelsize': 16,\n",
    "          'axes.titlesize': 18\n",
    "         }\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98113857-23e1-4fa1-9fae-bfea6dc06e83",
   "metadata": {},
   "source": [
    "Let's fetch the data we want to look at. We'll need to change the code below depending on what dataset we're inspecting (and however you've stored it on your system)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c76a0c0-3615-417a-941f-91245e263e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputpath = path_prefix+'data/pion_converted/'\n",
    "drpath = os.getcwd() + '/dr/pion_converted/'\n",
    "\n",
    "datafiles = {\n",
    "    'p0':inputpath + 'user.angerami.mc16_13TeV.900246.PG_singlepi0_logE0p2to2000.e8312_e7400_s3170_r12383.v01-45-gaa27bcb_OutputStream/*.root',\n",
    "    'pp':inputpath + 'user.angerami.mc16_13TeV.900247.PG_singlepion_logE0p2to2000.e8312_e7400_s3170_r12383.v01-45-gaa27bcb_OutputStream/*.root'\n",
    "}\n",
    "\n",
    "drfiles = {key: val.replace(inputpath, drpath) for key,val in datafiles.items()}\n",
    "\n",
    "cluster_branches = [\n",
    "    'cluster_E', 'cluster_E_LCCalib', \n",
    "    'cluster_Pt', 'cluster_Eta', 'cluster_Phi', \n",
    "    'cluster_nCells',\n",
    "    'cluster_ENG_CALIB_TOT', 'cluster_EM_PROBABILITY',\n",
    "]\n",
    "\n",
    "\n",
    "dr_branches = [\n",
    "    'cluster_E', 'cluster_Eta', 'cluster_Phi', 'cluster_ENG_CALIB_TOT',\n",
    "    'truth_E', 'truth_Eta', 'truth_Phi', 'truth_PdgId',\n",
    "    'dR'\n",
    "]\n",
    "\n",
    "event_branches = [\n",
    "    'truthPartPt',\n",
    "    'truthPartE',\n",
    "    'truthPartEta',\n",
    "    'truthPartPhi',\n",
    "    'nCluster',\n",
    "    'clusterCount'\n",
    "]\n",
    "\n",
    "colors = {\n",
    "    'pp':rt.kRed,\n",
    "    'p0':plotstyle.curve\n",
    "}\n",
    "\n",
    "keys = list(datafiles.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067610c2-b00d-4bf6-b7dc-593948145140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: branch_filter is not working\n",
    "\n",
    "cluster_tree = {\n",
    "    key:ur.lazy(':'.join((val,'ClusterTree')), branch_filter=lambda x: x.name in cluster_branches)\n",
    "    for key,val in datafiles.items()\n",
    "}\n",
    "\n",
    "# We have some info on the minimum delta R between clusters and (stable) truth-level particles,\n",
    "# tucked away in another set of files produced by our deltaR.py/deltaR.C script.\n",
    "dr_tree = {\n",
    "    key:ur.lazy(':'.join((val,'dr_info')), branch_filter=lambda x: x.name in dr_branches)\n",
    "    for key,val in drfiles.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a543d4-f678-4fef-adb0-11897640acb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform some cuts on the data\n",
    "indices = {\n",
    "    key:np.full(len(val),True,dtype=bool)\n",
    "    for key,val in cluster_tree.items()\n",
    "}\n",
    "\n",
    "cuts = {\n",
    "    'eta':('cluster_Eta','window',(-0.7,0.7)),\n",
    "    'energy':('cluster_ENG_CALIB_TOT','lower',0.2)\n",
    "}\n",
    "\n",
    "for cutname, cut in cuts.items():\n",
    "    var, cuttype, cutval = cut\n",
    "    \n",
    "    for key in keys:\n",
    "        if cuttype == 'lower': sel = cluster_tree[key][var] > cutval\n",
    "        elif cuttype == 'upper': sel = cluster_tree[key][var] < cutval\n",
    "        elif cuttype == 'window':sel = (cluster_tree[key][var] > cutval[0]) * (cluster_tree[key][var] < cutval[1])\n",
    "        else: continue\n",
    "        indices[key] *= sel.to_numpy()\n",
    "        \n",
    "cluster_tree = {key:val[indices[key]] for key,val in cluster_tree.items()}\n",
    "dr_tree      = {key:val[indices[key]] for key,val in dr_tree.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89eb7f68-d611-4636-9fac-5922a24da12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check that things line up\n",
    "#assert(np.sum(cluster_tree['pp']['cluster_E'] - dr_tree['pp']['cluster_E']) == 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1007eee-3980-4aca-8fea-49492f255feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convenience functions for histogramming, from numpy to ROOT.\n",
    "# Much faster than looping over data for sequential filling of histograms.\n",
    "def hist1d(x,h):\n",
    "    N = h.GetNbinsX()\n",
    "    bin_i = h.GetBinLowEdge(1)\n",
    "    bin_f = h.GetBinLowEdge(N) + h.GetBinWidth(N)    \n",
    "    np_hist = np.histogram(x, bins=N, range=(bin_i, bin_f))[0]\n",
    "    for i in range(N):\n",
    "        h.SetBinContent(i+1, np_hist[i])\n",
    "    return\n",
    "\n",
    "def hist2d(x,y,h):\n",
    "    \n",
    "    Nx = h.GetNbinsX()\n",
    "    Ny = h.GetNbinsY()\n",
    "    \n",
    "    bin_xi = h.GetXaxis().GetBinLowEdge(1)\n",
    "    bin_xf = h.GetXaxis().GetBinLowEdge(Nx) + h.GetXaxis().GetBinWidth(Nx)\n",
    "    \n",
    "    bin_yi = h.GetYaxis().GetBinLowEdge(1)\n",
    "    bin_yf = h.GetYaxis().GetBinLowEdge(Ny) + h.GetYaxis().GetBinWidth(Ny)\n",
    "      \n",
    "    np_hist = np.histogram2d(x,y, bins=(Nx,Ny), range=((bin_xi,bin_xf),(bin_yi, bin_yf)))[0]\n",
    "    for i in range(Nx):\n",
    "        for j in range(Ny):\n",
    "            h.SetBinContent(i+1,j+1,np_hist[i,j])\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91cbc88-7ab6-4fe5-947d-cf266f9784cb",
   "metadata": {},
   "source": [
    "## Plotting reco vs truth energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674efddc-687b-44b5-ac95-1230eae855e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, axs = plt.subplots(1,2,figsize=(24,12))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a0037e-8fe4-4547-822b-68166e5424b8",
   "metadata": {},
   "source": [
    "## Plotting average cluster images\n",
    "\n",
    "Let's plot average images for each calo layer, for $\\pi^0$ and $\\pi^\\pm$.\n",
    "\n",
    "Specifically, let's calculate the average of *normalized* images. This means that we normalize each cluster's set of images -- rescale them so the integral across calo layers is 1 -- and then average the results. This is very different than averaging and then normalizing, but given that our classifiers use normalized images (and perform the normalization on-the-fly) the average of norm'd images is meaningful.\n",
    "\n",
    "Furthermore, we can compare this average to the average images in the old datasets (where normalization was applied).\n",
    "\n",
    "To accomplish this, we're going to use some fancy `uproot` stuff, specifically its `iterate` functionality. This seems more practical than just using the lazy arrays we made earlier, as they're pretty big and doing reductions on them is quite slow/intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80468b1d-ff6d-4388-8c50-8833b5a93658",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfile = 'avg_images.root'\n",
    "cell_threshold = None # TODO: Allow us to exclude cells below some threshold value\n",
    "\n",
    "if(not pathlib.Path(rfile).exists()):\n",
    "\n",
    "    f = rt.TFile(rfile,\"RECREATE\")\n",
    "    \n",
    "    step_size = 1000 # number of entries to read in each iteration\n",
    "    loop_keys = layers + ['cluster_ENG_CALIB_TOT','cluster_Eta']\n",
    "\n",
    "    # Numpy buffers for images\n",
    "    avg_images = {\n",
    "        key:{\n",
    "            layer:np.zeros(cell_shapes[layer])\n",
    "            for layer in layers\n",
    "        }\n",
    "        for key in keys\n",
    "    }\n",
    "    \n",
    "    # Histograms for images\n",
    "    im_hists = {\n",
    "        key:{\n",
    "            layer:rt.TH2F(\n",
    "                qu.RN(),\n",
    "                '{} ({});#eta;#phi'.format(layer, pi_latex[key]),\n",
    "                cell_shapes[layer][0],\n",
    "                -0.5 * cell_shapes[layer][0] * cell_widths[layer][0],\n",
    "                 0.5 * cell_shapes[layer][0] * cell_widths[layer][0],\n",
    "                cell_shapes[layer][1],\n",
    "                -0.5 * cell_shapes[layer][1] * cell_widths[layer][1],\n",
    "                 0.5 * cell_shapes[layer][1] * cell_widths[layer][1]\n",
    "            )\n",
    "            for layer in layers\n",
    "        }\n",
    "        for key in keys\n",
    "    }\n",
    "\n",
    "    n = 0\n",
    "    for key,val in datafiles.items():\n",
    "        for array in ur.iterate(':'.join((val,'ClusterTree')), loop_keys, step_size=step_size):\n",
    "\n",
    "            # Apply cuts\n",
    "            #TODO: Unify this with cuts applied earlier? Or go back to the existing lazy arrays?\n",
    "            indices = np.full(len(array),False,bool)\n",
    "\n",
    "            sel = (array['cluster_ENG_CALIB_TOT'] > 0.2).to_numpy()\n",
    "            sel *= (np.abs(array['cluster_Eta'].to_numpy()) < 0.7)\n",
    "            array = array[sel]\n",
    "            n += len(array)\n",
    "\n",
    "            ims = {layer: array[layer].to_numpy() for layer in layers}\n",
    "            norms = {layer: np.sum(x,axis=(1,2)) for layer,x in ims.items()}\n",
    "            norms = np.sum([x for x in norms.values()],axis=0)\n",
    "            norms[norms == 0.] = 1.\n",
    "            ims = {layer: np.divide(ims[layer], norms.reshape(-1,1,1)) for layer in ims.keys()}\n",
    "\n",
    "            for layer in layers:\n",
    "                avg_images[key][layer] += np.sum(ims[layer],axis=0)    \n",
    "                \n",
    "        for layer in layers:\n",
    "            avg_images[key][layer] /= n\n",
    "            \n",
    "            for i in range(cell_shapes[layer][0]):\n",
    "                for j in range(cell_shapes[layer][1]):\n",
    "                    im_hists[key][layer].SetBinContent(i+1,j+1,avg_images[key][layer][i,j])\n",
    "            \n",
    "            im_hists[key][layer].Write('{}_{}'.format(key,layer))            \n",
    "    f.Close()\n",
    "            \n",
    "f = rt.TFile(rfile,\"READ\")\n",
    "    # Histograms for images\n",
    "im_hists = {\n",
    "    key:{\n",
    "        layer: f.Get('{}_{}'.format(key,layer))\n",
    "        for layer in layers\n",
    "    }\n",
    "    for key in keys\n",
    "}\n",
    "\n",
    "for key in keys:\n",
    "    for layer in layers:\n",
    "        im_hists[key][layer].SetDirectory(0)\n",
    "f.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419e0aff-159f-481d-8f31-df38f58e89e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rt.gStyle.SetOptStat(0)\n",
    "c = rt.TCanvas(qu.RN(),qu.RN(),2400,2400)\n",
    "c.Divide(3,4)\n",
    "i = 0\n",
    "for key in keys:\n",
    "    for layer in layers:\n",
    "        c.cd(i+1)\n",
    "        im_hists[key][layer].Draw('COLZ')\n",
    "        i += 1\n",
    "c.Draw()\n",
    "c.SaveAs('avg_ims.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaba4489-a456-4d02-879d-b36c2b8a4007",
   "metadata": {},
   "source": [
    "## Plotting Cluster and cell energies\n",
    "\n",
    "Let's look at the energies of individual cells, i.e. pixels in our calorimeter images.\n",
    "\n",
    "Note that as our dataset has *many* clusters, and each cluster has *many* pixels (in multi-dim awkward arrays), preparing the histograms is very slow for the full dataset. To deal with this, we will load the histogram from a ROOT file if it exists -- so that we only have to compute the histograms once.\n",
    "\n",
    "Another potential solution would be to do this plotting in ROOT/C++, much like how we handle the computation of $\\Delta R$. But keeping this in Python and dealing with a little slowness might be easier.\n",
    "\n",
    "We'll also display the histograms using `matplotlib` so that we can use the \"symlog\" axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5631e7cb-69fb-462c-ba73-306556b08640",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfile = 'cell_energies.root'\n",
    "\n",
    "if(not pathlib.Path(rfile).exists()):\n",
    "    # rng = np.random.default_rng()\n",
    "    # suppression_factor = 1000\n",
    "    h1 = {\n",
    "        layer:{\n",
    "            key:rt.TH1F(qu.RN(),'{} ({});Energy;% Count'.format(layer,key), int(8e4),-40.,40.)\n",
    "            for key in keys\n",
    "        }\n",
    "        for layer in layers\n",
    "    }\n",
    "\n",
    "    for key in keys:\n",
    "        n = len(cluster_tree[key])\n",
    "    #     idx = np.arange(n)\n",
    "        msk = np.full(n,True,dtype=bool)\n",
    "    #     idx = rng.choice(idx,int(n/suppression_factor),replace=False)\n",
    "    #     msk[idx] = True\n",
    "    #     print(np.sum(msk))\n",
    "        for layer in layers:\n",
    "            hist1d(cluster_tree[key][layer][msk].to_numpy().flatten(), h1[layer][key])\n",
    "            h1[layer][key].Scale(1./h1[layer][key].Integral())\n",
    "\n",
    "    f = rt.TFile(rfile,'RECREATE')\n",
    "    for layer in layers:\n",
    "        for key in keys:\n",
    "            f.cd()\n",
    "            h1[layer][key].Write('{}_{}'.format(key,layer))\n",
    "    f.Close()\n",
    "\n",
    "f = rt.TFile(rfile,'READ')\n",
    "h1 = {}\n",
    "for layer in layers:\n",
    "    h1[layer] = {}\n",
    "    for key in keys:\n",
    "        h1[layer][key] = f.Get('{}_{}'.format(key,layer))\n",
    "        h1[layer][key].SetDirectory(0) # copy to memory, so that we can close our ROOT file\n",
    "f.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77583b1b-4a23-419d-9f7e-849ba357c870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hist2XY(h):\n",
    "    N = h.GetNbinsX()\n",
    "    bin_i = h.GetBinLowEdge(1)\n",
    "    bin_f = h.GetBinLowEdge(N) + h.GetBinWidth(N)\n",
    "    \n",
    "    x_vals = np.linspace(bin_i, bin_f, N)\n",
    "    y_vals = np.array([h.GetBinContent(i+1) for i in range(N)])\n",
    "    return x_vals, y_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f93fe6c-dcc1-4ac0-a981-3327ee5ca804",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,3,figsize=(24,12))\n",
    "\n",
    "for i,key in enumerate(keys):\n",
    "    for j,layer in enumerate(layers):\n",
    "        ax = axs.flatten()[j]\n",
    "        x,y = Hist2XY(h1[layer][key])\n",
    "        ax.plot(x,y,label=pi_latex_plt[key])\n",
    "\n",
    "for i,ax in enumerate(axs.flatten()):\n",
    "    ax.set_xscale('symlog', linthresh=2.0e-2)\n",
    "    ax.set_yscale('log')\n",
    "    \n",
    "    ax.set_xlim(-1.0e-2,1.0e2)\n",
    "    \n",
    "    ax.set_title(layers[i])\n",
    "    ax.set_xlabel('Cell Energy [GeV]')\n",
    "    ax.set_ylabel('% Count')\n",
    "    \n",
    "    ax.legend()\n",
    "    plotstyle.SetStylePlt(ax)\n",
    "\n",
    "fig.savefig('cell_energies.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6923ce90-8632-4c86-9a2d-e14864ade502",
   "metadata": {},
   "source": [
    "## Plotting $\\Delta R$ (versus Energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6f38bb-c5c1-42e3-9243-e52be9374a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1D histograms\n",
    "h1 = {\n",
    "    'dR':{\n",
    "        key:rt.TH1F(qu.RN(),'{};#Delta R;% Count'.format(pi_latex[key]),120,0.,6.) \n",
    "        for key in keys\n",
    "    },\n",
    "    'cluster_E':{\n",
    "        key:rt.TH1F(qu.RN(),'{};E;% Count'.format(pi_latex[key]),2500,0.,2500.) \n",
    "        for key in keys\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "h2 = {\n",
    "    key:rt.TH2F(qu.RN(),'{};#Delta R;Energy [GeV];% Count'.format(pi_latex[key]),240,0.,6., 2500,0.,2500.) \n",
    "    for key in keys\n",
    "}\n",
    "\n",
    "for key in keys:\n",
    "    n = len(cluster_tree[key])\n",
    "    hist1d(dr_tree[key]['dR'].to_numpy(), h1['dR'][key])\n",
    "    hist1d(cluster_tree[key]['cluster_E'].to_numpy(), h1['cluster_E'][key])\n",
    "    \n",
    "    hist2d(\n",
    "        dr_tree[key]['dR'].to_numpy(), \n",
    "        cluster_tree[key]['cluster_E'].to_numpy(), \n",
    "        h2[key]\n",
    "    )\n",
    "    \n",
    "    for h in h1.values():\n",
    "        h[key].Scale(1./h[key].Integral())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15703126-a16a-45d0-b651-e40ae8907efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rt.gStyle.SetOptStat(0)\n",
    "c1 = rt.TCanvas(qu.RN(),'c1',1600,600)\n",
    "c2 = rt.TCanvas(qu.RN(),'c2',1600,600)\n",
    "c1.Divide(2,1)\n",
    "c2.Divide(2,1)\n",
    "margin = 0.2\n",
    "\n",
    "c1.cd(1)\n",
    "hstack_a = rt.THStack()\n",
    "legend = rt.TLegend(0.7,0.7,0.9,0.9)\n",
    "legend.SetTextColor(plotstyle.text)\n",
    "for key in keys: \n",
    "    h1['dR'][key].SetLineColor(colors[key])\n",
    "    hstack_a.Add(h1['dR'][key])\n",
    "    legend.AddEntry(h1['dR'][key],pi_latex[key],'l')\n",
    "hstack_a.SetTitle('Min. #Delta R (truth, cluster);#Delta R;% Count')\n",
    "hstack_a.Draw('NOSTACK HIST')\n",
    "legend.Draw()\n",
    "rt.gPad.SetLogx()\n",
    "rt.gPad.SetLogy()\n",
    "\n",
    "c1.cd(2)\n",
    "hstack_b = rt.THStack()\n",
    "for key in keys: \n",
    "    h1['cluster_E'][key].SetLineColor(colors[key])\n",
    "    hstack_b.Add(h1['cluster_E'][key])\n",
    "hstack_b.SetTitle('cluster_E;E [GeV];% Count')\n",
    "hstack_b.Draw('NOSTACK HIST')\n",
    "legend.Draw()\n",
    "rt.gPad.SetLogx()\n",
    "rt.gPad.SetLogy()\n",
    "\n",
    "c2.cd()\n",
    "for i,key in enumerate(keys):\n",
    "    c2.cd(i+1)\n",
    "    rt.gPad.SetRightMargin(margin)\n",
    "    h2[key].Draw('COLZ')\n",
    "    #colorbar = h2[key].GetListOfFunctions().FindObject('palette')\n",
    "    #colorbar.SetX1NDC(1.- 0.9 * margin)\n",
    "    #colorbar.SetX2NDC(1.- 0.7 * margin)\n",
    "    rt.gPad.SetLogx()\n",
    "    rt.gPad.SetLogy()\n",
    "    rt.gPad.SetLogz()\n",
    "    \n",
    "c1.Draw()\n",
    "c2.Draw()\n",
    "\n",
    "c1.SaveAs('dr.png')\n",
    "c2.SaveAs('dr_e.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5c7842-40ad-4ad3-8126-588f3fb5b6dc",
   "metadata": {},
   "source": [
    "Let's also look at the PDG Id's of the nearest truth particles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151ba3cc-f960-45f6-a30c-a37736b963d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdg_codes = {\n",
    "    11 : 'e^{#pm}',\n",
    "    22 : '#gamma',\n",
    "    111 : '#pi^{0}',\n",
    "    211: '#pi^{#pm}'\n",
    "}\n",
    "\n",
    "pdg_vals = {\n",
    "    11: 0,\n",
    "    22: 1,\n",
    "    111: 2,\n",
    "    211: 3\n",
    "}\n",
    "\n",
    "pdg_bar_chart = {\n",
    "    key: rt.TH1F(qu.RN(),'Truth match;Particle;% Count',4,0,4)\n",
    "    for key in keys\n",
    "}\n",
    "\n",
    "for key,chart in pdg_bar_chart.items():\n",
    "    vals = np.abs(dr_tree[key]['truth_PdgId'].to_numpy())\n",
    "    vals = np.array([pdg_vals[x] for x in vals])\n",
    "    hist1d(vals,chart)\n",
    "    chart.Scale(1./chart.Integral())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e915ef-2eaa-4ab1-b9a0-0f9d0793ae2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = rt.TCanvas(qu.RN(),'c',800,600)\n",
    "legend = rt.TLegend(0.8,0.8,0.9,0.9)\n",
    "legend.SetTextColor(plotstyle.text)\n",
    "stack = rt.THStack()\n",
    "stack.SetTitle('Truth match;Particle;% Count')\n",
    "for key,chart in pdg_bar_chart.items(): \n",
    "    chart.SetLineColor(colors[key])\n",
    "    chart.SetFillColorAlpha(colors[key],0.3)\n",
    "    stack.Add(chart)\n",
    "    legend.AddEntry(chart,pi_latex[key],'lf')\n",
    "\n",
    "stack.Draw('NOSTACK HIST')\n",
    "legend.Draw()\n",
    "rt.gPad.SetLogy()\n",
    "stack.SetMaximum(3. * stack.GetMaximum())\n",
    "rt.gPad.SetGrid()\n",
    "\n",
    "stack.GetXaxis().SetNdivisions(-4)\n",
    "stack.GetXaxis().SetLabelSize(0.05)\n",
    "for i,code in enumerate(pdg_codes.values()):\n",
    "    stack.GetXaxis().SetBinLabel(i+1,code)\n",
    "\n",
    "c.Draw()\n",
    "c.SaveAs('particle_id.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa41fff-60ab-4ca7-a83f-19c2619a7772",
   "metadata": {},
   "source": [
    "We see that for charged pions, the truth particles are always the truth particles themselves -- this makes sense (the truth particles probably come from the generator level, *before* the hadronic showering).\n",
    "\n",
    "For the neutral pions, we have a mix of gammas, electrons/positrons and neutral pions themselves. The gammas completely dominate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
