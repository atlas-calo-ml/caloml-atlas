{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # For quickly testing when working on external libraries\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TopoCluster Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a slightly different take on regression than what we've commonly being doing. Instead of making a network that will output the predicted topo-cluster energy, we will make one that will output the predicted *ratio* between true and reco energies.\n",
    "\n",
    "What's the difference? It might sound like a trivial change, but the ratio is a unitless quantity. So this might help with the problem of having to cover many orders of magnitude in outputs. Now, our \"classic\" regression networks were dealing with this by performing some scaling of inputs:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "E_\\text{reco}  &\\rightarrow \\ln{ \\left (E_\\text{reco} \\right)} \\;, \\\\\n",
    "E_\\text{truth}  &\\rightarrow \\ln{ \\left (E_\\text{truth} \\right)} \\;\\; \\text{(target)},\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The output of the network, $x$, is then converted to the predicted energy $E_\\text{pred}$ via the inverse transformation:\n",
    "\n",
    "$$\n",
    "x \\rightarrow e^{x} = E_\\text{pred}\n",
    "$$\n",
    "\n",
    "Thus if there's some small error in $x$, $x \\rightarrow x + \\delta x$, this may result in a large error in $E_\\text{pred}$:\n",
    "\n",
    "$$\n",
    "e^{x + \\delta x} = e^{\\delta x} E_\\text{pred} \\; .\n",
    "$$\n",
    "\n",
    "For example, if we're trying to get an energy of $100 \\, \\text{[GeV]}$ for a particular cluster, but our network ouput ($x$) is $10\\%$ too large, this will lead to a $\\sim 50\\%$ error in $E_\\text{pred}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML fitting/loading/saving settings\n",
    "overwriteModel = False # If true, force training. If false, load the specified model if it already exists.\n",
    "\n",
    "finishTraining = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's choose our training data (and associated strategy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'pion'\n",
    "subdir = 'pion3' # name for subdir holding models/plots\n",
    "h5_name_suffix = 'tdata_60GeV' # name for HDF5 files containing selected training/validation/testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we can define some training settings here for our different models. Note that we'll need to expand this if we add new models to the list.\n",
    "\n",
    "Also note that some model-specific settings are only adjusted further down in the code, where each model is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_settings = {\n",
    "    'all':{'lr':1.0e-5, 'epochs':200, 'patience':40, 'batch_size':200, 'skip':False},\n",
    "    'simple':{'lr':5.0e-5, 'epochs':200, 'patience':40, 'batch_size':200, 'skip':False},\n",
    "    'simple_cnn':{'lr':1.0e-4, 'epochs':200, 'patience':40, 'batch_size':200, 'skip':False},\n",
    "    'split_emb_cnn':{'lr':1.0e-4, 'epochs':200, 'patience':40, 'batch_size':200, 'skip':False},\n",
    "    'resnet':{'lr':1.0e-5, 'epochs':200, 'patience':40, 'batch_size':200, 'skip':True}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.24/02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-28 11:10:00.067672: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "# Import some basic libraries.\n",
    "import sys, os, glob, pathlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import uproot as ur\n",
    "#import ROOT as rt # optionally used for plotting\n",
    "#import joblib as jl # for saving scalers\n",
    "\n",
    "# Import our resolution utilities\n",
    "path_prefix = os.getcwd() + '/../'\n",
    "if(path_prefix not in sys.path): sys.path.append(path_prefix)\n",
    "from util import resolution_util as ru\n",
    "from util import plot_util       as pu\n",
    "from util import ml_util         as mu\n",
    "from util import qol_util        as qu\n",
    "from util import io_util         as iu\n",
    "\n",
    "# Custom tensorflow.keras callbacks\n",
    "from util.keras.callbacks import GetCallbacks\n",
    "\n",
    "# Regression-specific utils\n",
    "from util.regression import data_util as rdu\n",
    "from util.regression import plot_util as rpu\n",
    "from util.regression import training_util as rtu\n",
    "\n",
    "#rt.gStyle.SetOptStat(0)\n",
    "# use our custom dark style for plots\n",
    "plotstyle = qu.PlotStyle('dark')\n",
    "plotstyle.SetStyle() # still need to manually adjust legends, paves\n",
    "\n",
    "plotpath = path_prefix + 'regression2/Plots/{}/'.format(subdir)\n",
    "modelpath = path_prefix + 'regression2/Models/{}/'.format(subdir)\n",
    "paths = [plotpath, modelpath]\n",
    "for path in [plotpath, modelpath]:\n",
    "    try: os.makedirs(path)\n",
    "    except: pass\n",
    "\n",
    "# metadata\n",
    "layers = [\"EMB1\", \"EMB2\", \"EMB3\", \"TileBar0\", \"TileBar1\", \"TileBar2\"]\n",
    "cell_size_phi = [0.098, 0.0245, 0.0245, 0.1, 0.1, 0.1]\n",
    "cell_size_eta = [0.0031, 0.025, 0.05, 0.1, 0.1, 0.2]\n",
    "len_phi = [4, 16, 16, 4, 4, 4]\n",
    "len_eta = [128, 16, 8, 4, 4, 2]\n",
    "cell_shapes = {layers[i]:(len_eta[i],len_phi[i]) for i in range(len(layers))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fancy display names for each pion type\n",
    "pi_latex = {\n",
    "    'p0': '\\(\\pi^{0}\\)',\n",
    "    'pp': '\\(\\pi^{\\pm}\\)',\n",
    "}\n",
    "pi_text = {\n",
    "    'p0': 'pi0',\n",
    "    'pp': 'pi+/-'\n",
    "}\n",
    "\n",
    "# Plotting settings\n",
    "# xkcd -- turn this on for fun-looking (but marginally less useful) plots\n",
    "use_xkcd = False\n",
    "if(use_xkcd):\n",
    "    mode = 'light'\n",
    "    plt.xkcd(scale=.75,length=100,randomness=1)\n",
    "    \n",
    "# plotting style -- manages our color palette and object colors\n",
    "mode = 'dark' # for publications, use \"light\"\n",
    "plotstyle = qu.PlotStyle(mode)\n",
    "    \n",
    "# some matplotlib-specific stuff\n",
    "params = {'legend.fontsize': 13,\n",
    "          'axes.labelsize': 18}\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Get the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me lay out some definitions, so it's clear as to what the data is.\n",
    "\n",
    "We have a number of different \"strategies\" (the `source` variable at the top). These correspond to different choices of training, validation and testing datasets.\n",
    "\n",
    "1. `pion`: We train and validate the network using our single pion data.\n",
    "\n",
    "2. `pion_legacy`: We train and validate using our old, noise-free single pion data.\n",
    "\n",
    "3. `pion_reweighted`: This is the same as `pion`, except that our training data is reweighted using a jet dataset (via their reco topo-cluster $p_T$ distributions), that corresponds with QCD dijet events.\n",
    "\n",
    "The validation performed for these networks is effectively being done on some \"holdout\" dataset from training -- it will by definition have similar kinematics, being drawn from the same set of events. The more interesting test -- how our energy regression performs in tandem with classification on our *unlabeled* jet dataset, will be handled in a separate notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(source == 'pion_legacy'):\n",
    "    inputpath = path_prefix+'data/pion_legacy/'\n",
    "    rootfiles = {\n",
    "        'p0':inputpath + 'pi0.root',\n",
    "        'pp':inputpath + 'pi[pm]*.root'\n",
    "    }\n",
    "    \n",
    "    branches = ['clusterE', 'clusterPt', 'clusterEta', 'cluster_ENG_CALIB_TOT']\n",
    "\n",
    "elif(source == 'pion' or source == 'pion_reweighted'):\n",
    "    inputpath=path_prefix+'data/pion/'\n",
    "    rootfiles = {        \n",
    "        'p0':inputpath + 'user.mswiatlo.900246.PG_singlepi0_logE0p2to2000.recon.ESD.e8312_e7400_s3170_r12383.images_v01.1_OutputStream/*.root',\n",
    "        'pp':inputpath + 'user.mswiatlo.900247.PG_singlepion_logE0p2to2000.recon.ESD.e8312_e7400_s3170_r12383.images_v01.1_OutputStream/*.root'\n",
    "    }\n",
    "    \n",
    "    branches = ['clusterE', 'clusterPt', 'clusterEta', 'cluster_ENG_CALIB_TOT']\n",
    "\n",
    "else: assert(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pandas DataFrame and calo images from /local/home/jano/ml4pions/LCStudies/regression2/../data/pion/tdata_60GeV_frame.h5 and /local/home/jano/ml4pions/LCStudies/regression2/../data/pion/tdata_60GeV_images.h5.\n",
      "Number of pi0     events:     600000\t(50.0%)\n",
      "Number of pi+/-   events:     600000\t(50.0%)\n",
      "Total: 1200000\n",
      "Loading indices for key p0 from /local/home/jano/ml4pions/LCStudies/regression2/../data/pion/tdata_60GeV_indices.h5.\n",
      "Loading indices for key pp from /local/home/jano/ml4pions/LCStudies/regression2/../data/pion/tdata_60GeV_indices.h5.\n"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "h5_name = inputpath + h5_name_suffix\n",
    "\n",
    "pdata,pcells = mu.setupPionData(\n",
    "    rootfiles, \n",
    "    branches=branches, \n",
    "    layers=layers, \n",
    "    balance_data=True, \n",
    "    n_max = 600000, # we usually use 300k\n",
    "    verbose=True,\n",
    "    load=True,\n",
    "    save=True,\n",
    "    filename=h5_name,\n",
    "    match_distribution='cluster_ENG_CALIB_TOT',\n",
    "    match_binning = (20000,0.,2000.),\n",
    "    cut_distributions=['cluster_ENG_CALIB_TOT','clusterEta'],\n",
    "    cut_values = [(.2,60.), (-0.7,0.7)],\n",
    "    cut_types=['window','window'] # use only lower cut for \"standard\" training data\n",
    ")\n",
    "\n",
    "total = np.sum([len(x) for x in pdata.values()],dtype=int)\n",
    "for key,frame in pdata.items():\n",
    "    n = len(frame)\n",
    "    print(\"Number of {a:<7} events: {b:>10}\\t({c:.1f}%)\".format(a=pi_text[key], b = n, c = 100. * n / total))\n",
    "print(\"Total: {}\".format(total))\n",
    "\n",
    "# Create/get training/validation/testing indices.\n",
    "pdata = rdu.DataPrep(pdata,\n",
    "                     trainfrac=0.7,\n",
    "                     filename=h5_name\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining regression variables\n",
    "\n",
    "Beyond the information already present in the data, we may want to pre-compute some extra regression inputs.\n",
    "\n",
    "The difference between computing them here, on the whole dataset, versus doing it in batch as part of the network itself, is that we can also define some `scalers` based on these variables. This will allow us to scale them across the dataset, for example to get them into the interval of \\[0,1\\]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 1.\n",
    "b = 1.\n",
    "EnergyMapping = iu.LinLogMapping(b=b,m=m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our `LinLogMapping` is pretty numerically stable as long as $b \\gg m$. I think this is the expected behaviour, given the instabilities with logarithms that we've seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some regression vars.\n",
    "# Note that the mapping functions can probably be sped up with numba, but might throw errors during plotting above. Need to look into this.\n",
    "for key,frame in pdata.items():\n",
    "    frame['logE'] = EnergyMapping.Forward(frame['clusterE'].to_numpy()) # log of reco energy, possible network input\n",
    "    frame['clusterEtaAbs'] = np.abs(frame['clusterEta'].to_numpy()) # absolute value of eta, possible network input\n",
    "    #frame['logECalib'] = EnergyMapping.Forward(frame['cluster_ENG_CALIB_TOT'].to_numpy()) # No longer our regression target\n",
    "\n",
    "    # Now get the ratio of \"truth\" to reco energy. Be mindful of zeros in denominator\n",
    "    x = frame['clusterE'].to_numpy()\n",
    "    x[x==0] = 1.\n",
    "    frame['ratioE'] = frame['cluster_ENG_CALIB_TOT'].to_numpy() / x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_file = modelpath + 'scalers.save'\n",
    "scaler_branches = ['logE', 'clusterEtaAbs']\n",
    "scalers = mu.setupScalers(pdata, scaler_branches, scaler_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make plots of any vars that are rescaled using our StandardScalers, to see what the rescaling has done. (We won't put axis labels since the units are kind of funny anyway...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's make a histogram of the regression vars, before and after scaling.\n",
    "rvars = ['logE', 's_logE']\n",
    "ranges = {\n",
    "    'logE':(120,-2.,10.),\n",
    "    's_logE':(100,-2.,2.)\n",
    "}\n",
    "fig, ax = plt.subplots(1,len(rvars),figsize=(7.5 * len(rvars),5))\n",
    "if(type(ax) != np.ndarray): ax = [ax]\n",
    "\n",
    "for i,rvar in enumerate(rvars):\n",
    "    vals = {}\n",
    "    for key,frame in pdata.items():\n",
    "        vals[key] = frame[rvar].to_numpy()\n",
    "    pu.histogramOverlay(ax[i], vals.values(), list(vals.keys()), rvar, 'Fractional count',\n",
    "                        x_min = ranges[rvar][1], x_max = ranges[rvar][2], xbins = ranges[rvar][0],\n",
    "                        normed = True, y_log = True,\n",
    "                        ps = plotstyle\n",
    "                       )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also make a plot of our regression target, to see what the distribution looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 540x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's make a histogram of the regression vars, before and after scaling.\n",
    "rvars = ['ratioE']\n",
    "ranges = {\n",
    "    'ratioE':(200,0.,20.),\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(1,len(rvars),figsize=(7.5 * len(rvars),5))\n",
    "if(type(ax) != np.ndarray): ax = [ax]\n",
    "\n",
    "for i,rvar in enumerate(rvars):\n",
    "    vals = {}\n",
    "    for key,frame in pdata.items():\n",
    "        vals[key] = frame[rvar].to_numpy()\n",
    "    pu.histogramOverlay(ax[i], vals.values(), list(vals.keys()), rvar, 'Fractional count',\n",
    "                        x_min = ranges[rvar][1], x_max = ranges[rvar][2], xbins = ranges[rvar][0],\n",
    "                        normed = True, y_log = True,\n",
    "                        ps = plotstyle\n",
    "                       )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we might have suspected, there is more spread for $\\pi^\\pm$ than for $\\pi^0$. In other words, the charged pion response is worse, and so it will make for a harder regression target. This is consistent with what we've seen elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may optionally perform some re-weighting of our training events. If using the `pion_reweighted` strategy, we will re-weight our single-pion training data to match the topo-cluster $p_T$ spectrum of our jet data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: consider re-adding some sample weighting schemes\n",
    "sample_weights = {\n",
    "    key: np.full(np.sum(frame['train'].to_numpy()),1.)\n",
    "    for key,frame in pdata.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow/Keras Prep\n",
    "\n",
    "In this workflow we have the ability to train a number of models -- some will require additional data setup. Here, we have some basic setup they will all use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Number of devices: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-28 11:11:26.912130: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2021-07-28 11:11:26.972886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:18:00.0 name: Quadro P5000 computeCapability: 6.1\n",
      "coreClock: 1.7335GHz coreCount: 20 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 269.00GiB/s\n",
      "2021-07-28 11:11:26.972961: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-07-28 11:11:26.977495: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2021-07-28 11:11:26.977630: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-07-28 11:11:26.978844: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2021-07-28 11:11:26.979284: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2021-07-28 11:11:26.981322: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2021-07-28 11:11:26.982235: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-07-28 11:11:26.982495: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-07-28 11:11:26.984451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2021-07-28 11:11:26.985830: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-07-28 11:11:26.991932: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:18:00.0 name: Quadro P5000 computeCapability: 6.1\n",
      "coreClock: 1.7335GHz coreCount: 20 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 269.00GiB/s\n",
      "2021-07-28 11:11:26.996169: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2021-07-28 11:11:26.996316: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-07-28 11:11:27.678906: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-07-28 11:11:27.678942: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
      "2021-07-28 11:11:27.678948: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
      "2021-07-28 11:11:27.680677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15549 MB memory) -> physical GPU (device: 0, name: Quadro P5000, pci bus id: 0000:18:00.0, compute capability: 6.1)\n"
     ]
    }
   ],
   "source": [
    "models = {} # keep track of the models -- note that we train multiple instances of each model\n",
    "histories = {} # keep track of histories of the models we train\n",
    "regressors = {} # keep track of the trained models (will be KerasRegressor objects)\n",
    "energy_ratio_names = {} # keep track of the names of regressed variables\n",
    "\n",
    "model_filename_suffixes = {\n",
    "    'pp':'_charged',\n",
    "    'p0':'_neutral'\n",
    "}\n",
    "\n",
    "energy_name_prefix = 'ratio_pred_'\n",
    "\n",
    "#from keras.wrappers.scikit_learn import KerasRegressor # scikit_learn wrapper -- why do we use this, vs. native tf.keras approach like in classification notebook?\n",
    "#from tensorflow.keras.models import load_model\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # disable some of the tensorflow info printouts, only display errors\n",
    "import tensorflow as tf\n",
    "#strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\"])\n",
    "#ngpu = strategy.num_replicas_in_sync\n",
    "#print ('Number of devices: {}'.format(ngpu))\n",
    "ngpu = 1\n",
    "\n",
    "#from util.regression.models import baseline_nn_All_model, simple_dnn, resnet, resnet_wide\n",
    "from util.regression.models import baseline_nn_model, depth_network, simple_cnn, split_emb_cnn, resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training \"all\" model\n",
    "\n",
    "Here we train a simple, fully-connected neural network that uses the calorimeter cells as input, along with reco energy and $\\eta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting baseline model.\n",
      "Preparing combined input:  |\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m| 100.0% Complete\n",
      "all: pi0\n",
      "Epoch 1/2\n",
      "2100/2100 [==============================] - 11s 5ms/step - loss: 0.0883 - mae: 0.1264 - mse: 0.0883 - val_loss: 0.1178 - val_mae: 0.1029 - val_mse: 0.1178\n",
      "Epoch 2/2\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.0691 - mae: 0.1072 - mse: 0.0691 - val_loss: 0.0884 - val_mae: 0.1057 - val_mse: 0.0884\n",
      "3000/3000 [==============================] - 5s 2ms/step\n",
      "all: pi+/-\n",
      "Epoch 1/2\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.5879 - mae: 0.2936 - mse: 0.5879 - val_loss: 20.7918 - val_mae: 0.2684 - val_mse: 20.7918\n",
      "Epoch 2/2\n",
      "2100/2100 [==============================] - 9s 5ms/step - loss: 2.8749 - mae: 0.2493 - mse: 2.8749 - val_loss: 6.9903 - val_mae: 0.2810 - val_mse: 6.9903\n",
      "3000/3000 [==============================] - 5s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-28 11:14:41.646991: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-07-28 11:14:41.666172: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2100000000 Hz\n",
      "2021-07-28 11:14:42.064468: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2021-07-28 11:14:42.386448: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n"
     ]
    }
   ],
   "source": [
    "# Baseline model\n",
    "model_key = 'all'\n",
    "skip = training_settings[model_key]['skip']\n",
    "if(not skip):\n",
    "    print('Starting baseline model.')\n",
    "    \n",
    "    # Data preparation\n",
    "    All_input = rdu.CombinedInput(pdata,\n",
    "                                  pcells,\n",
    "                                  branches = ['s_logE','s_clusterEtaAbs']\n",
    "                                 )\n",
    "    # ----------------------------------------------------------------\n",
    "\n",
    "    energy_ratio_names[model_key] = energy_name_prefix + model_key\n",
    "    lr = training_settings[model_key]['lr']\n",
    "    decay = 0. # lr decay *within* each epoch\n",
    "    dropout = -1. # < 0 -> no dropout\n",
    "    models[model_key] = baseline_nn_model(lr=lr, decay=decay, dropout=dropout)\n",
    "\n",
    "    # Set our training hyper-parameters.\n",
    "    batch_size = training_settings[model_key]['batch_size']\n",
    "    epochs = training_settings[model_key]['epochs']\n",
    "    gamma = .1 # lr decay between epochs (via scheduler)\n",
    "    patience = training_settings[model_key]['patience']\n",
    "    min_delta = 0.0005\n",
    "    verbose = 1\n",
    "    regressors[model_key] = {}\n",
    "    histories[model_key] = {}\n",
    "\n",
    "    # Load/train the models, and evaluate them on all the data.\n",
    "    for key in All_input.keys():\n",
    "        print('{}: {}'.format(model_key, pi_text[key]))\n",
    "\n",
    "        tidx = pdata[key]['train']\n",
    "        vidx = pdata[key]['val'] \n",
    "\n",
    "        model_dir = ''.join([modelpath, model_key])\n",
    "        model_filename = '{}/{}{}.h5'.format(model_dir,model_key,model_filename_suffixes[key])\n",
    "\n",
    "        regressors[model_key][key], histories[model_key][key] = rtu.TrainNetwork(\n",
    "            model=models[model_key],\n",
    "            modelfile = model_filename,\n",
    "            x_train = All_input[key][tidx],\n",
    "            y_train = pdata[key]['ratioE'][tidx],\n",
    "            x_valid = All_input[key][vidx],\n",
    "            y_valid = pdata[key]['ratioE'][vidx],\n",
    "            sample_weight=sample_weights[key],\n",
    "            callbacks = GetCallbacks(model_filename, append=True, use_decay=True, gamma=gamma, min_delta=min_delta, patience=patience), \n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=verbose,\n",
    "            overwriteModel=overwriteModel,\n",
    "            finishTraining=finishTraining\n",
    "        )\n",
    "\n",
    "        # Get predictions for all the data.\n",
    "        pdata[key][energy_ratio_names[model_key]] = rtu.GetPredictions(regressor=regressors[model_key][key],\n",
    "                                                                       model_input = All_input[key],\n",
    "                                                                       indices = {k:pdata[key][k].to_numpy() for k in ['train','val','test']},\n",
    "                                                                       truth = pdata[key]['cluster_ENG_CALIB_TOT'].to_numpy(),\n",
    "                                                                       reco = pdata[key]['clusterE'].to_numpy(),\n",
    "                                                                       scaler = None,\n",
    "                                                                       mapping = None,\n",
    "                                                                       filename = model_filename.replace('.h5','_output.h5')\n",
    "                                                                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training \"simple\" (depth-based) model\n",
    "\n",
    "Here we train another simple, fully-connected neural network that uses reco energy and $\\eta$ as input, along with depth information (vector of integrals of calorimeter images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting simple (depth-based) model.\n",
      "simple: pi0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8725/28916787.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mmodel_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'{}/{}{}.h5'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_filename_suffixes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         regressors[model_key][key], histories[model_key][key] = rtu.TrainNetwork(\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mmodelfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_filename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml4pions/LCStudies/regression2/../util/regression/training_util.py\u001b[0m in \u001b[0;36mTrainNetwork\u001b[0;34m(model, modelfile, x_train, y_train, x_valid, y_valid, sample_weight, callbacks, epochs, batch_size, verbose, overwriteModel, finishTraining)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# and the final model didn't reach the specified last epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinishTraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         history = regressor.fit(\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml4p/lib/python3.8/site-packages/tensorflow/python/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0mfit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml4p/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1131\u001b[0m          \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRespectCompiledTrainableState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m       \u001b[0;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m       data_handler = data_adapter.get_data_handler(\n\u001b[0m\u001b[1;32m   1134\u001b[0m           \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m           \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml4p/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1362\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1364\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml4p/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_verify_data_adapter_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m     self._adapter = adapter_cls(\n\u001b[0m\u001b[1;32m   1155\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml4p/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;31m# If sample_weights are not specified for an output use 1.0 as weights.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m     (sample_weights, _, _) = training_utils.handle_partial_sample_weights(\n\u001b[0m\u001b[1;32m    253\u001b[0m         y, sample_weights, sample_weight_modes, check_all_flat=True)\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml4p/lib/python3.8/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mhandle_partial_sample_weights\u001b[0;34m(outputs, sample_weights, sample_weight_modes, check_all_flat)\u001b[0m\n\u001b[1;32m     77\u001b[0m   any_sample_weight = sample_weights is not None and any(\n\u001b[1;32m     78\u001b[0m       w is not None for w in sample_weights)\n\u001b[0;32m---> 79\u001b[0;31m   partial_sample_weight = any_sample_weight and any(\n\u001b[0m\u001b[1;32m     80\u001b[0m       w is None for w in sample_weights)\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml4p/lib/python3.8/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     77\u001b[0m   any_sample_weight = sample_weights is not None and any(\n\u001b[1;32m     78\u001b[0m       w is not None for w in sample_weights)\n\u001b[0;32m---> 79\u001b[0;31m   partial_sample_weight = any_sample_weight and any(\n\u001b[0m\u001b[1;32m     80\u001b[0m       w is None for w in sample_weights)\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml4p/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   7008\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_limit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7009\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7010\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7011\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7012\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml4p/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml4p/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_slice_helper\u001b[0;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[1;32m   1013\u001b[0m       \u001b[0m_check_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       \u001b[0mbegin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1015\u001b[0;31m       \u001b[0mend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1016\u001b[0m       \u001b[0mstrides\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       \u001b[0mshrink_axis_mask\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m<<\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Simple model\n",
    "model_key = 'simple'\n",
    "skip = training_settings[model_key]['skip']\n",
    "\n",
    "if(not skip):\n",
    "\n",
    "    print('Starting simple (depth-based) model.')\n",
    "    \n",
    "    # Data preparation.\n",
    "    All_input = rdu.DepthInput(pdata,\n",
    "                               pcells,\n",
    "                               branch_map = {\n",
    "                                   's_logE':'energy',\n",
    "                                   's_clusterEtaAbs':'eta'\n",
    "                               }\n",
    "                              )\n",
    "    Split_input = rdu.DictionarySplit(All_input, pdata)\n",
    "    train_input, valid_input = Split_input['train'], Split_input['val']\n",
    "    # ----------------------------------------------------------------\n",
    "    \n",
    "    energy_ratio_names[model_key] = energy_name_prefix + model_key\n",
    "    lr = training_settings[model_key]['lr']\n",
    "    units = 32\n",
    "    depth = 8\n",
    "    decay = 0. # lr decay *within* each epoch\n",
    "    dropout = 0.05 # < 0 -> no dropout\n",
    "    models[model_key] = depth_network(lr=lr, decay=decay, dropout=dropout, units=units, depth=depth)\n",
    "\n",
    "    # Set our training hyper-parameters.\n",
    "    batch_size = training_settings[model_key]['batch_size']\n",
    "    epochs = training_settings[model_key]['epochs']\n",
    "    gamma = .1 # lr decay between epochs (via scheduler)\n",
    "    patience = training_settings[model_key]['patience']\n",
    "    min_delta = 0.0005\n",
    "    verbose = 1\n",
    "    regressors[model_key] = {}\n",
    "    histories[model_key] = {}\n",
    "\n",
    "    # Load/train the models, and evaluate them on all the data.\n",
    "    for key in All_input.keys():\n",
    "        print('{}: {}'.format(model_key, pi_text[key]))\n",
    "        tidx = pdata[key]['train']\n",
    "        vidx = pdata[key]['val']  \n",
    "\n",
    "        model_dir = ''.join([modelpath, model_key])\n",
    "        model_filename = '{}/{}{}.h5'.format(model_dir,model_key,model_filename_suffixes[key])\n",
    "\n",
    "        regressors[model_key][key], histories[model_key][key] = rtu.TrainNetwork(\n",
    "            model=models[model_key],\n",
    "            modelfile = model_filename,\n",
    "            x_train = train_input[key],\n",
    "            y_train = pdata[key]['ratioE'][tidx],\n",
    "            x_valid = valid_input[key],\n",
    "            y_valid = pdata[key]['ratioE'][vidx],\n",
    "            sample_weight=sample_weights[key],\n",
    "            callbacks = GetCallbacks(model_filename, append=True, use_decay=True, gamma=gamma, min_delta=min_delta, patience=patience), \n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=verbose,\n",
    "            overwriteModel=overwriteModel,\n",
    "            finishTraining=finishTraining\n",
    "        )\n",
    "\n",
    "        # Get predictions for all the data.\n",
    "        pdata[key][energy_ratio_names[model_key]] = rtu.GetPredictions(regressor=regressors[model_key][key],\n",
    "                                                                       model_input = All_input[key],\n",
    "                                                                       indices = {k:pdata[key][k].to_numpy() for k in ['train','val','test']},\n",
    "                                                                       truth = pdata[key]['cluster_ENG_CALIB_TOT'].to_numpy(),\n",
    "                                                                       reco = pdata[key]['clusterE'].to_numpy(),\n",
    "                                                                       scaler = None,\n",
    "                                                                       mapping = None,\n",
    "                                                                       filename = model_filename.replace('.h5','_output.h5')\n",
    "                                                                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple CNN\n",
    "\n",
    "This network uses some convolutions of the EMB layers, together with energy, eta and depth information (the depth info is computed internally)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple CNN\n",
    "model_key = 'simple_cnn'\n",
    "skip = training_settings[model_key]['skip']\n",
    "\n",
    "if(not skip):\n",
    "    print('Starting simple CNN model.')\n",
    "    # Data preparation.\n",
    "    All_input = rdu.ResnetInput(pdata,\n",
    "                                pcells,\n",
    "                                branch_map = {\n",
    "                                    's_logE':'energy',\n",
    "                                    's_clusterEtaAbs':'eta'\n",
    "                                }\n",
    "    )\n",
    "    Split_input = rdu.DictionarySplit(All_input, pdata)\n",
    "    train_input, valid_input = Split_input['train'], Split_input['val']\n",
    "    # ----------------------------------------------------------------\n",
    "\n",
    "    energy_ratio_names[model_key] = energy_name_prefix + model_key\n",
    "    lr = training_settings[model_key]['lr']\n",
    "    decay = 0. # lr decay *within* each epoch\n",
    "    dropout = 0.2\n",
    "    augmentation = True # whether or not to augment data during training, by flipping in eta & phi\n",
    "    models[model_key] = simple_cnn(lr=lr, decay=decay, dropout=dropout, augmentation=augmentation)\n",
    "\n",
    "    # Set our training hyper-parameters.\n",
    "    batch_size = training_settings[model_key]['batch_size']\n",
    "    epochs = training_settings[model_key]['epochs']\n",
    "    gamma = .1 # lr decay between epochs (via scheduler)\n",
    "    patience = training_settings[model_key]['patience']\n",
    "    min_delta = 0.0005\n",
    "    verbose = 1\n",
    "    regressors[model_key] = {}\n",
    "    histories[model_key] = {}\n",
    "\n",
    "    # Load/train the models, and evaluate them on all the data.\n",
    "    for key in All_input.keys():\n",
    "        print('{}: {}'.format(model_key, pi_text[key]))\n",
    "        tidx = pdata[key]['train']\n",
    "        vidx = pdata[key]['val']  \n",
    "\n",
    "        model_dir = ''.join([modelpath, model_key])\n",
    "        model_filename = '{}/{}{}.h5'.format(model_dir,model_key,model_filename_suffixes[key])\n",
    "\n",
    "        regressors[model_key][key], histories[model_key][key] = rtu.TrainNetwork(\n",
    "            model=models[model_key],\n",
    "            modelfile = model_filename,\n",
    "            x_train = train_input[key],\n",
    "            y_train = pdata[key]['ratioE'][tidx],\n",
    "            x_valid = valid_input[key],\n",
    "            y_valid = pdata[key]['ratioE'][vidx],\n",
    "            sample_weight=sample_weights[key],\n",
    "            callbacks = GetCallbacks(model_filename, append=True, use_decay=True, gamma=gamma, min_delta=min_delta, patience=patience), \n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=verbose,\n",
    "            overwriteModel=overwriteModel,\n",
    "            finishTraining=finishTraining\n",
    "        )\n",
    "\n",
    "        # Get predictions for all the data.\n",
    "        pdata[key][energy_ratio_names[model_key]] = rtu.GetPredictions(regressor=regressors[model_key][key],\n",
    "                                                                       model_input = All_input[key],\n",
    "                                                                       indices = {k:pdata[key][k].to_numpy() for k in ['train','val','test']},\n",
    "                                                                       truth = pdata[key]['cluster_ENG_CALIB_TOT'].to_numpy(),\n",
    "                                                                       reco = pdata[key]['clusterE'].to_numpy(),\n",
    "                                                                       scaler = None,\n",
    "                                                                       mapping = None,\n",
    "                                                                       filename = model_filename.replace('.h5','_output.h5')\n",
    "                                                                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split EMB CNN\n",
    "\n",
    "We can train a CNN model that's inspired by one of our well-performing CNN classifiers. It groups the 6 calo layers as EMB1, EMB2+EMB3, and TileBar0+TileBar1+TileBar2. Compared to the classifier, we've removed a few convolutions to make it a little simpler, and have added in energy and abs(eta) information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split EMB CNN (i.e. a more complicated CNN)\n",
    "model_key = 'split_emb_cnn'\n",
    "skip = training_settings[model_key]['skip']\n",
    "\n",
    "if(not skip):\n",
    "    print('\"Starting split EMB\" CNN model.')\n",
    "    \n",
    "    # Data preparation\n",
    "    All_input = rdu.ResnetInput(pdata,\n",
    "                                pcells,\n",
    "                                branch_map = {\n",
    "                                    's_logE':'energy',\n",
    "                                    's_clusterEtaAbs':'eta'\n",
    "                                }\n",
    "    )\n",
    "    Split_input = rdu.DictionarySplit(All_input, pdata)\n",
    "    train_input, valid_input = Split_input['train'], Split_input['val']\n",
    "    # ----------------------------------------------------------------\n",
    "    \n",
    "    energy_ratio_names[model_key] = energy_name_prefix + model_key\n",
    "    lr = training_settings[model_key]['lr']\n",
    "    decay = 0. # lr decay *within* each epoch\n",
    "    dropout = 0.2\n",
    "    augmentation = True # whether or not to augment data during training, by flipping in eta & phi\n",
    "    models[model_key] = split_emb_cnn(lr=lr, decay=decay, dropout=dropout, augmentation=augmentation)\n",
    "\n",
    "    # Set our training hyper-parameters.\n",
    "    batch_size = training_settings[model_key]['batch_size']\n",
    "    epochs = training_settings[model_key]['epochs']\n",
    "    gamma = .1 # lr decay between epochs (via scheduler)\n",
    "    patience = training_settings[model_key]['patience']\n",
    "    min_delta = 0.0005\n",
    "    verbose = 1\n",
    "    regressors[model_key] = {}\n",
    "    histories[model_key] = {}\n",
    "\n",
    "    # Load/train the models, and evaluate them on all the data.\n",
    "    for key in All_input.keys():\n",
    "        print('{}: {}'.format(model_key, pi_text[key]))\n",
    "        tidx = pdata[key]['train']\n",
    "        vidx = pdata[key]['val']  \n",
    "\n",
    "        model_dir = ''.join([modelpath, model_key])\n",
    "        model_filename = '{}/{}{}.h5'.format(model_dir,model_key,model_filename_suffixes[key])\n",
    "\n",
    "        regressors[model_key][key], histories[model_key][key] = rtu.TrainNetwork(\n",
    "            model=models[model_key],\n",
    "            modelfile = model_filename,\n",
    "            x_train = train_input[key],\n",
    "            y_train = pdata[key]['ratioE'][tidx],\n",
    "            x_valid = valid_input[key],\n",
    "            y_valid = pdata[key]['ratioE'][vidx],\n",
    "            sample_weight=sample_weights[key],\n",
    "            callbacks = GetCallbacks(model_filename, append=True, use_decay=True, gamma=gamma, min_delta=min_delta, patience=patience), \n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=verbose,\n",
    "            overwriteModel=overwriteModel,\n",
    "            finishTraining=finishTraining\n",
    "        )\n",
    "\n",
    "        # Get predictions for all the data.\n",
    "        pdata[key][energy_ratio_names[model_key]] = rtu.GetPredictions(regressor=regressors[model_key][key],\n",
    "                                                                       model_input = All_input[key],\n",
    "                                                                       indices = {k:pdata[key][k].to_numpy() for k in ['train','val','test']},\n",
    "                                                                       truth = pdata[key]['cluster_ENG_CALIB_TOT'].to_numpy(),\n",
    "                                                                       reco = pdata[key]['clusterE'].to_numpy(),\n",
    "                                                                       scaler = None,\n",
    "                                                                       mapping = None,\n",
    "                                                                       filename = model_filename.replace('.h5','_output.h5')\n",
    "                                                                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet\n",
    "\n",
    "We can also train an implementation of ResNet. More precisely, we use a ResNet model on the calorimeter images, and then mix in the energy and $\\eta$ at the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_key = 'resnet'\n",
    "skip = training_settings[model_key]['skip']\n",
    "\n",
    "if(not skip):\n",
    "\n",
    "    print('Starting ResNet.')\n",
    "    # Data preparation -- here we can possibly take a shortcut if this data was already prepared.\n",
    "    if(training_settings['split_emb_cnn']['skip']):\n",
    "        All_input = rdu.ResnetInput(pdata,\n",
    "                                    pcells,\n",
    "                                    branch_map = {\n",
    "                                        's_logE':'energy',\n",
    "                                        's_clusterEtaAbs':'eta'\n",
    "                                    }\n",
    "        )\n",
    "        Split_input = rdu.DictionarySplit(All_input, pdata)\n",
    "        train_input, valid_input = Split_input['train'], Split_input['val']\n",
    "    # ----------------------------------------------------------------    \n",
    "    \n",
    "    energy_ratio_names[model_key] = energy_name_prefix + model_key\n",
    "    lr = training_settings[model_key]['lr']\n",
    "    decay = 0. # lr decay *within* each epoch 1e-6\n",
    "    channels = 6\n",
    "    filter_sets = [\n",
    "        [64,64,256],\n",
    "        [128,128,512]\n",
    "        #[256,256,1024],\n",
    "        #[512,512,2048]\n",
    "    ]         \n",
    "    f_vals = [3,3] # [3,3,3,3] sizes of filters in middle of conv/identity blocks\n",
    "    s_vals = [1,2] # [1,2,2,2] strides for each convolutional block\n",
    "    i_vals = [2,3] # [2,3,5,2] number of identity blocks per stage\n",
    "    input_shape = (128,16)\n",
    "    augmentation = True # whether or not to augment data during training, by flipping in eta & phi\n",
    "\n",
    "    models[model_key] = resnet(lr=lr, channels=channels, filter_sets=filter_sets, f_vals=f_vals, s_vals=s_vals, i_vals=i_vals, decay=decay, input_shape=input_shape, augmentation=augmentation)\n",
    "\n",
    "    # Set our training hyper-parameters.\n",
    "    batch_size = training_settings[model_key]['batch_size']\n",
    "    epochs = training_settings[model_key]['epochs']\n",
    "    gamma = .1 # lr decay between epochs (via scheduler)\n",
    "    patience = training_settings[model_key]['patience']\n",
    "    min_delta = 0.0005\n",
    "    verbose = 1\n",
    "    regressors[model_key] = {}\n",
    "    histories[model_key] = {}\n",
    "\n",
    "    # Load/train the models, and evaluate them on all the data.\n",
    "    for key in All_input.keys():\n",
    "        print('{}: {}'.format(model_key, pi_text[key]))\n",
    "        tidx = pdata[key]['train']\n",
    "        vidx = pdata[key]['val']  \n",
    "\n",
    "        model_dir = ''.join([modelpath, model_key])\n",
    "        model_filename = '{}/{}{}.h5'.format(model_dir,model_key,model_filename_suffixes[key])\n",
    "\n",
    "        regressors[model_key][key], histories[model_key][key] = rtu.TrainNetwork(\n",
    "            model=models[model_key],\n",
    "            modelfile = model_filename,\n",
    "            x_train = train_input[key],\n",
    "            y_train = pdata[key]['ratioE'][tidx],\n",
    "            x_valid = valid_input[key],\n",
    "            y_valid = pdata[key]['ratioE'][vidx],\n",
    "            sample_weight=sample_weights[key],\n",
    "            callbacks = GetCallbacks(model_filename, append=True, use_decay=True, gamma=gamma, min_delta=min_delta, patience=patience), \n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=verbose,\n",
    "            overwriteModel=overwriteModel,\n",
    "            finishTraining=finishTraining\n",
    "        )\n",
    "\n",
    "        # Get predictions for all the data.\n",
    "        pdata[key][energy_ratio_names[model_key]] = rtu.GetPredictions(regressor=regressors[model_key][key],\n",
    "                                                                       model_input = All_input[key],\n",
    "                                                                       indices = {k:pdata[key][k].to_numpy() for k in ['train','val','test']},\n",
    "                                                                       truth = pdata[key]['cluster_ENG_CALIB_TOT'].to_numpy(),\n",
    "                                                                       reco = pdata[key]['clusterE'].to_numpy(),\n",
    "                                                                       scaler = None,\n",
    "                                                                       mapping = None,\n",
    "                                                                       filename = model_filename.replace('.h5','_output.h5')\n",
    "                                                                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet Mini\n",
    "\n",
    "In an effort to simplify our ResNet -- and get it to train faster -- we can consider just using a single channel for our images, or some subset of channels. We can re-use the input we prepared for our full ResNet, though we'll only need a portion of it. Note that if we only use a single channel, the rescaling (via `input_shape`) is redundant, and we should just set that to the original dimensions (and, in practice, remove the scaling entirely if we stick with just one channel).\n",
    "\n",
    "**TODO:** Reimplement this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting results (testing how well our network works)\n",
    "\n",
    "Now, let's plot some kinematics and network results. We'll make two groups of plots -- one for charged pions and one for neutral pions.\n",
    "\n",
    "Within each group of plots, we'll make two plots for each quantity -- one made using just the training data, and then one made using all the data (training + whatever we excluded -- but still excluding events with `cluster_ENG_CALIB_TOT` $< 0$ since these blow up network output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for convenience below\n",
    "training_frames = {key:frame[frame['train']] for key,frame in pdata.items()}\n",
    "validation_frames = {key:frame[frame['val']] for key,frame in pdata.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make plots of mse and mae for training and validation (if present)\n",
    "\n",
    "for model_key in histories.keys():\n",
    "    for dkey in histories[model_key].keys():\n",
    "    \n",
    "        dname = dkey\n",
    "        \n",
    "        epochs = len(histories[model_key][dkey]['mae'])\n",
    "        epoch_ticks = epochs\n",
    "        if(epoch_ticks > 10): epoch_ticks = epoch_ticks/2\n",
    "        x = np.arange(epochs) + 1\n",
    "        fig, ax = plt.subplots(1,2,figsize=(15,5))\n",
    "    \n",
    "        keys = ['mae','val_mae']\n",
    "        lines = [histories[model_key][dkey][key] for key in keys]\n",
    "        pu.multiplot_common(ax[0], x,lines, keys, y_min=1.0e-3, y_max=10., y_log=True, x_ticks=epoch_ticks, xlabel = 'Epoch', ylabel = 'MAE', title='Mean Avg. Error for {} ({})'.format(model_key,dname), ps=plotstyle)\n",
    "    \n",
    "        keys = ['mse','val_mse']\n",
    "        lines = [histories[model_key][dkey][key] for key in keys]\n",
    "        pu.multiplot_common(ax[1], x,lines, keys, y_min=1.0e-3, y_max=10., y_log=True, x_ticks=epoch_ticks, xlabel = 'Epoch', ylabel = 'MSE', title='Mean Sq. Error for {} ({})'.format(model_key,dname), ps=plotstyle)\n",
    "    \n",
    "        # add grids\n",
    "        for axis in ax.flatten():\n",
    "            axis.grid(True,color=plotstyle.grid_plt)\n",
    "\n",
    "        qu.SaveSubplots(fig, ax, ['mae_{}_{}'.format(model_key,dkey), 'mse_{}_{}'.format(model_key,dkey)], savedir=plotpath)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
